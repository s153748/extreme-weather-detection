{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MASTER THESIS PROJECT**\n",
    "\n",
    "*Identification and Exploration of Extreme Weather Events From Twitter Data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T14:54:38.412369Z",
     "start_time": "2021-04-24T14:54:38.393272Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "import re\n",
    "import ast \n",
    "from ast import literal_eval\n",
    "import json\n",
    "import os\n",
    "import string  \n",
    "import math\n",
    "import random\n",
    "import calendar\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyterthemes import jtplot\n",
    "import seaborn as sns\n",
    "from seaborn import color_palette\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "import collections\n",
    "from googletrans import Translator\n",
    "import altair as alt \n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from tqdm import tqdm\n",
    "import folium\n",
    "from folium import FeatureGroup, LayerControl, plugins, Map, Marker\n",
    "from folium.plugins import FastMarkerCluster, MarkerCluster\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk import word_tokenize\n",
    "from gensim import models\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding, SimpleRNN\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.backend import clear_session\n",
    "import spacy\n",
    "from spacymoji import Emoji\n",
    "import keplergl\n",
    "from keplergl import KeplerGl\n",
    "import dash\n",
    "import dash_bootstrap_components as dbc\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from jupyter_dash import JupyterDash\n",
    "from dash.dependencies import Input, Output, ClientsideFunction, State\n",
    "from dash.exceptions import PreventUpdate\n",
    "import pathlib\n",
    "from flask import request\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "#from plotly.subplots import make_subplots\n",
    "from ipywidgets import widgets\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T10:05:52.620509Z",
     "start_time": "2021-04-24T10:05:50.500460Z"
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv('data/final_labelled_tweets.csv')\n",
    "\n",
    "df.dropna(subset=['tokens'],inplace=True)\n",
    "df['tokens'] = [literal_eval(s) for s in df['tokens']]\n",
    "for i in range(len(df)):\n",
    "    try:\n",
    "        df['geo'][i] = eval(df['geo'][i])\n",
    "    except:\n",
    "        df['geo'][i] = np.nan\n",
    "    try:\n",
    "        df['place'][i] = eval(df['place'][i])\n",
    "    except:\n",
    "        df['place'][i] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CrisislexT6: https://crisislex.org/data-collections.html#CrisisLexT6\n",
    "\n",
    "1. Alberta floods https://en.wikipedia.org/wiki/2013_Alberta_floods \n",
    "2. Queensland floods https://en.wikipedia.org/wiki/Cyclone_Oswald\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:01:20.866201Z",
     "start_time": "2021-04-18T14:01:17.266868Z"
    }
   },
   "outputs": [],
   "source": [
    "# load json with tweets\n",
    "floods = ['2013_Alberta_floods','2013_Queensland_floods'] # use cases\n",
    "\n",
    "tweets_df = pd.DataFrame()\n",
    "for flood in floods:\n",
    "\n",
    "    df1 = pd.read_json(f'data/CrisisLexT6/{flood}_ids.json', lines=True)\n",
    "    # rename\n",
    "    df2 = df1.rename(columns={'id': 'tweet_id','is_quote_status':'quoted', 'in_reply_to_status_id':'reply_tweet_id'})\n",
    "    # add variable for area\n",
    "    area = flood.split('_')[1]\n",
    "    df2['area'] = len(df2)*[area]\n",
    "    tweets_df = pd.concat([tweets_df, df2])\n",
    "\n",
    "# drop irrelevant columns\n",
    "tweets_df.drop(['id_str','possibly_sensitive','entities','extended_entities','contributors','display_text_range','truncated','in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'in_reply_to_screen_name'], axis=1, inplace=True)\n",
    "\n",
    "print(f'Number of tweets: {len(tweets_df)}')\n",
    "tweets_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:01:20.887845Z",
     "start_time": "2021-04-18T14:01:20.868230Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:01:20.986958Z",
     "start_time": "2021-04-18T14:01:20.890538Z"
    }
   },
   "outputs": [],
   "source": [
    "# load csv with labels for tweets\n",
    "labels_df = pd.DataFrame()\n",
    "\n",
    "for flood in floods:\n",
    "    df1 = pd.read_csv(f'data/CrisisLexT6/{flood}.csv')\n",
    "    df1['tweet_id'] = [int(t[1]) for t in df1['tweet id'].str.split(\"'\")]\n",
    "    labels_df = pd.concat([labels_df, df1])\n",
    "\n",
    "labels_df = labels_df[labels_df.columns[2:]]\n",
    "labels_df = labels_df.rename(columns={' label': 'relevant'})\n",
    "labels_df = labels_df.replace('off-topic', 0).replace('on-topic', 1)\n",
    "labels_df = labels_df[labels_df['tweet_id'].isin(tweets_df['tweet_id'])]\n",
    "\n",
    "print(f'Number of tweets: {len(labels_df)}')\n",
    "labels_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:01:21.275526Z",
     "start_time": "2021-04-18T14:01:20.989877Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge tweets with their labels \n",
    "df_join = pd.merge(tweets_df, labels_df, on='tweet_id', how='inner').reset_index(drop=True)\n",
    "df_join['user_id'] = [df_join['user'][i]['id'] for i in range(len(df_join))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:01:21.283984Z",
     "start_time": "2021-04-18T14:01:21.277660Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_join['user'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:01:21.773390Z",
     "start_time": "2021-04-18T14:01:21.285530Z"
    }
   },
   "outputs": [],
   "source": [
    "# get user information\n",
    "users = [df_join['user'][i] for i in range(len(df_join))]\n",
    "df_users = pd.DataFrame(users)\n",
    "cols = ['id', 'name', 'screen_name', 'location', 'description', 'url', 'followers_count', \n",
    "        'friends_count', 'favourites_count', 'verified', 'statuses_count']\n",
    "df_users = df_users[cols].drop_duplicates().reset_index(drop=True)\n",
    "df_users = df_users.rename(columns={'id': 'user_id', 'name': 'user_name', 'screen_name': 'user_screen_name', \n",
    "                                    'location': 'user_location', 'description': 'user_description', 'url': 'user_url',\n",
    "                                    'followers_count':'user_followers_count', 'friends_count':'user_friends_count',\n",
    "                                    'favourites_count':'user_favourites_count', 'verified':'user_verified', 'statuses_count':'user_statuses_count'})\n",
    "df_users.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:01:22.460135Z",
     "start_time": "2021-04-18T14:01:22.383401Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge tweets with user information\n",
    "df = pd.merge(df_join, df_users, how='inner', on='user_id').reset_index(drop=True)\n",
    "df = df.replace(r'', np.NaN)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:01:23.082785Z",
     "start_time": "2021-04-18T14:01:23.049749Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:01:23.446966Z",
     "start_time": "2021-04-18T14:01:23.429693Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove duplicates of tweet_id\n",
    "try: \n",
    "    before = len(df)\n",
    "    df_drop = df.drop_duplicates(subset='tweet_id').reset_index(drop=True)\n",
    "    print(f'Number of duplicates removed: {before-len(df_drop)}')\n",
    "except:\n",
    "    print('No duplicates found')\n",
    "df = df_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:01:25.210253Z",
     "start_time": "2021-04-18T14:01:25.191388Z"
    }
   },
   "outputs": [],
   "source": [
    "# filter on English tweets only\n",
    "df = df[df['lang']=='en'].reset_index(drop=True)\n",
    "print(f'Number of English tweets: {len(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:02:59.689150Z",
     "start_time": "2021-04-18T14:01:26.726630Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenization of tweet text\n",
    "df_token = df\n",
    "df_token['tokens'] = \"\"\n",
    "\n",
    "# for tokenization of emojies\n",
    "nlp_spacymoji = spacy.load(\"en_core_web_sm\")\n",
    "emoji = Emoji(nlp_spacymoji, merge_spans=True)\n",
    "nlp_spacymoji.add_pipe(emoji, first=True)\n",
    "\n",
    "for i,content in enumerate(df_token['full_text']):\n",
    "    if content:\n",
    "        txt = content.translate(str.maketrans('', '', string.punctuation)) # remove punctuations\n",
    "        doc = nlp_spacymoji(txt) # handle emojies \n",
    "        text = [token.text for token in doc] # split to tokens\n",
    "        words1 = [t.lower() for t in text] # lower letters\n",
    "        sw = stopwords.words(\"english\") # set stopwords\n",
    "        words2 = [t for t in words1 if t not in sw] # remove stopwords\n",
    "        wordnet_lemmatizer = WordNetLemmatizer() # set lemmatizer\n",
    "        words3 = [wordnet_lemmatizer.lemmatize(t) for t in words2]  # lemmatize\n",
    "        words4 = [x for x in words3 if not any(c.isdigit() for c in x)] # remove words with numbers\n",
    "        \n",
    "        df_token['tokens'][i] = words4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:55:41.256536Z",
     "start_time": "2021-04-18T15:55:41.253454Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df_token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:55:41.474372Z",
     "start_time": "2021-04-18T15:55:41.467359Z"
    }
   },
   "outputs": [],
   "source": [
    "df.full_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:55:41.743045Z",
     "start_time": "2021-04-18T15:55:41.733830Z"
    }
   },
   "outputs": [],
   "source": [
    "df.tokens[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:55:41.881413Z",
     "start_time": "2021-04-18T15:55:41.870159Z"
    }
   },
   "outputs": [],
   "source": [
    "# check for NANs\n",
    "df['tokens'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:55:42.039411Z",
     "start_time": "2021-04-18T15:55:41.945392Z"
    }
   },
   "outputs": [],
   "source": [
    "# duplicates of full_text\n",
    "dupl = df[df.duplicated(subset='full_text',keep=False)].sort_values(\"full_text\")\n",
    "print(f'Number of duplicate tweets: {len(dupl)} corresponding to {np.round(len(dupl)/len(df)*100,1)} %')\n",
    "\n",
    "duplicates = dupl[['full_text']].groupby(dupl[['full_text']].columns.tolist()).size().reset_index().rename(columns={0:'duplicates'})\n",
    "duplicates.sort_values(\"duplicates\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:55:43.653443Z",
     "start_time": "2021-04-18T15:55:43.420529Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge duplicates counts on df\n",
    "df = pd.merge(df,duplicates,on='full_text',how='outer').reset_index(drop=True)\n",
    "df['duplicates'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Tweet data dictionary:* https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/tweet]\n",
    "\n",
    "Retweets: \n",
    "* `retweeted_status` Users can amplify the broadcast of Tweets authored by other users by retweeting. Retweets can be distinguished from typical Tweets by the existence of a retweeted_status attribute. This attribute contains a representation of the original Tweet that was retweeted. Note that retweets of retweets do not show representations of the intermediary retweet, but only the original Tweet. (Users can also unretweet a retweet they created by deleting their retweet.)\n",
    "* `retweeted` Indicates whether this Tweet has been Retweeted by the authenticating user (using the retweet button)\n",
    "* `retweet_count`: Number of times this Tweet has been retweeted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:55:43.798332Z",
     "start_time": "2021-04-18T15:55:43.793306Z"
    }
   },
   "outputs": [],
   "source": [
    "# number of tweets with retweeted as True\n",
    "len(df[df['retweeted']==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:55:44.028718Z",
     "start_time": "2021-04-18T15:55:44.017331Z"
    }
   },
   "outputs": [],
   "source": [
    "# number of tweets with retweeted_status\n",
    "df['retweeted_status'].fillna('none', inplace=True)\n",
    "len(df[df['retweeted_status']!='none'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:55:44.276246Z",
     "start_time": "2021-04-18T15:55:44.269079Z"
    }
   },
   "outputs": [],
   "source": [
    "df[['tweet_id','full_text','retweet_count','retweeted']].iloc[48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:55:44.504420Z",
     "start_time": "2021-04-18T15:55:44.492471Z"
    }
   },
   "outputs": [],
   "source": [
    "# retweeted_status: representation of the original Tweet that was retweeted\n",
    "df['retweeted_status'][48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:55:44.867437Z",
     "start_time": "2021-04-18T15:55:44.643461Z"
    }
   },
   "outputs": [],
   "source": [
    "# number of tweets with full_text starting with \"RT\"\n",
    "len([df['full_text'][i].startswith('RT') for i in range(len(df)) if df['full_text'][i].startswith('RT')==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:55:47.453391Z",
     "start_time": "2021-04-18T15:55:45.900171Z"
    }
   },
   "outputs": [],
   "source": [
    "# update retweeted variable and add original tweet id variable\n",
    "df['original_tweet_id'] = \"\"\n",
    "for i in range(len(df)):\n",
    "    if df['retweeted_status'][i]!='none': \n",
    "        df['retweeted'][i] = True\n",
    "        df['original_tweet_id'][i] = df['retweeted_status'][i]['id']\n",
    "    elif df['full_text'][i].startswith('RT')==True:\n",
    "        df['retweeted'][i] = True\n",
    "    else:\n",
    "        df['retweeted'][i] = False\n",
    "        \n",
    "num = sum(df['retweeted'])\n",
    "print(f'Number of retweets: {num} corresponding to {np.round(num/len(df)*100,1)} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:55:47.487335Z",
     "start_time": "2021-04-18T15:55:47.455657Z"
    }
   },
   "outputs": [],
   "source": [
    "# retweet count does not seem reliable as retweeted = False for cases with retweet_count > 0\n",
    "df[df['retweet_count']>0][['full_text','retweeted','retweeted_status','retweet_count']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quotes:\n",
    "* `quoted`: Indicates whether this is a Quoted Tweet\n",
    "\n",
    "Favorites:\n",
    "* `favorited`: Indicates whether this Tweet has been liked by the authenticating user\n",
    "* `favorite_count`: Indicates approximately how many times this Tweet has been liked by Twitter users\n",
    "\n",
    "Replies:\n",
    "* `reply_tweet_id`: Contains the integer representation of the original Tweet’s ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:55:48.177045Z",
     "start_time": "2021-04-18T15:55:48.165072Z"
    }
   },
   "outputs": [],
   "source": [
    "# number of tweets that are quoted\n",
    "print(len(df[df['quoted']==True]))\n",
    "df = df.drop(['quoted'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:55:48.324063Z",
     "start_time": "2021-04-18T15:55:48.311801Z"
    }
   },
   "outputs": [],
   "source": [
    "# number of tweets that are favorited\n",
    "print(len(df[df['favorited']==True]))\n",
    "df = df.drop(['favorited'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:55:48.457986Z",
     "start_time": "2021-04-18T15:55:48.445559Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(df[df['favorite_count']>0]))\n",
    "df = df.drop(['favorite_count'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:55:48.758307Z",
     "start_time": "2021-04-18T15:55:48.737977Z"
    }
   },
   "outputs": [],
   "source": [
    "# number of replies \n",
    "print(df['reply_tweet_id'].fillna('none', inplace=True))\n",
    "len(df[df['reply_tweet_id']!='none'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:56:02.123050Z",
     "start_time": "2021-04-18T15:56:00.652936Z"
    }
   },
   "outputs": [],
   "source": [
    "# add boolean variable for whether a tweet is a reply\n",
    "df['is_reply'] = \"\"\n",
    "for i in range(len(df)):\n",
    "    if df['reply_tweet_id'][i]!='none': \n",
    "        df['is_reply'][i] = True\n",
    "    else:\n",
    "        df['is_reply'][i] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:56:02.136950Z",
     "start_time": "2021-04-18T15:56:02.125021Z"
    }
   },
   "outputs": [],
   "source": [
    "# select numeric columns\n",
    "df_numeric = df.select_dtypes(include=[np.number])\n",
    "numeric_cols = df_numeric.columns.values\n",
    "print(f'Numeric variables:\\n {numeric_cols}')\n",
    "\n",
    "# select non numeric columns\n",
    "df_non_numeric = df.select_dtypes(exclude=[np.number])\n",
    "non_numeric_cols = df_non_numeric.columns.values\n",
    "print(f'Non-numeric variables:\\n {non_numeric_cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:56:02.191237Z",
     "start_time": "2021-04-18T15:56:02.139834Z"
    }
   },
   "outputs": [],
   "source": [
    "# find numbers and percentages of missing data\n",
    "print(f'Total number of missing values: {df.isnull().sum().sum()}\\n')\n",
    "\n",
    "print('Number and percentage of missing values per variable:')\n",
    "for col in df.columns:\n",
    "    pct_missing = np.mean(df[col].isnull())*100\n",
    "    if pct_missing > 0:\n",
    "        print(f'{col}: {df[col].isnull().sum()} - {np.round(pct_missing,2)} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:56:03.136061Z",
     "start_time": "2021-04-18T15:56:02.192913Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualize the missing data with a heatmap\n",
    "sns.set()\n",
    "plt.figure(figsize=(12, 6))\n",
    "heatmap = sns.heatmap(df[df.columns].isnull(), cmap=sns.color_palette(['#f7fbff', '#05264c']),cbar=False)\n",
    "heatmap.set_xlabel('columns',fontsize=14)\n",
    "heatmap.set_ylabel('rows',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:56:03.362590Z",
     "start_time": "2021-04-18T15:56:03.138172Z"
    }
   },
   "outputs": [],
   "source": [
    "# create missing indicator for variables with missing data\n",
    "for col in df.columns:\n",
    "    missing = df[col].isnull()\n",
    "    num_missing = np.sum(missing)\n",
    "    if num_missing > 0:  \n",
    "        df['{}_ismissing'.format(col)] = missing\n",
    "\n",
    "# based on the indicator, plot the bar chart of missing values\n",
    "ismissing_cols = [col for col in df.columns if 'ismissing' in col]\n",
    "df['num_missing'] = df[ismissing_cols].sum(axis=1)\n",
    "sns.set()\n",
    "df['num_missing'].value_counts().reset_index().sort_values(by='index').plot.bar(\n",
    "    x='index', y='num_missing', figsize=(12, 6), color='#05264c')\n",
    "plt.xticks(rotation=0)\n",
    "plt.xlabel('Number of missing values per obervation',fontsize=14)\n",
    "plt.ylabel('Number of obervations',fontsize=14)\n",
    "plt.legend('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:56:04.166782Z",
     "start_time": "2021-04-18T15:56:04.149484Z"
    }
   },
   "outputs": [],
   "source": [
    "# for non-numeric variables fill missing values with empty strings \n",
    "#for col in non_numeric_cols:\n",
    "#    missing = df[col].isnull()\n",
    "#    num_missing = np.sum(missing)\n",
    "#    if num_missing > 0:\n",
    "#        print('Filling missing values for: {}'.format(col))\n",
    "#        df['{}_ismissing'.format(col)] = missing\n",
    "#        df[col] = df[col].fillna('')\n",
    "\n",
    "# drop columns used to detect missing values\n",
    "df = df.drop(ismissing_cols, axis=1)\n",
    "df = df.drop('num_missing', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:56:32.339159Z",
     "start_time": "2021-04-18T15:56:05.415950Z"
    }
   },
   "outputs": [],
   "source": [
    "# check repetitiveness\n",
    "repetitive_cols = []\n",
    "print('Percentage of the same value per variable:\\n')\n",
    "for col in df.columns:\n",
    "    repetitive = (df[col].value_counts()/len(df.index)).iloc[0]\n",
    "    print('{0}: {1:.1f}%'.format(col, repetitive*100))\n",
    "    if repetitive > 0.8:\n",
    "        repetitive_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:56:32.380443Z",
     "start_time": "2021-04-18T15:56:32.341366Z"
    }
   },
   "outputs": [],
   "source": [
    "print('\\nTop values of most repetitive variables:')\n",
    "for col in repetitive_cols:\n",
    "    display(df[col].value_counts()[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:56:32.392440Z",
     "start_time": "2021-04-18T15:56:32.382756Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop variable with only one distinct value\n",
    "df = df.drop(['lang'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:57:10.893507Z",
     "start_time": "2021-04-18T15:57:10.857808Z"
    }
   },
   "outputs": [],
   "source": [
    "df['created_at'] = pd.to_datetime(df['created_at'], format='%Y-%m-%d')\n",
    "df['created_at_year'] = df['created_at'].dt.year\n",
    "df['created_at_month'] = df['created_at'].dt.month\n",
    "df['created_at_weekday'] = df['created_at'].dt.weekday\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:57:59.066006Z",
     "start_time": "2021-04-18T15:57:57.664940Z"
    }
   },
   "outputs": [],
   "source": [
    "# export to csv\n",
    "df.to_csv('data/final_labelled_tweets.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:23:49.888319Z",
     "start_time": "2021-04-18T15:23:47.921877Z"
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv('data/final_labelled_tweets.csv')\n",
    "\n",
    "df.dropna(subset=['tokens'],inplace=True)\n",
    "df['tokens'] = [literal_eval(s) for s in df['tokens']]\n",
    "for i in range(len(df)):\n",
    "    try:\n",
    "        df['geo'][i] = eval(df['geo'][i])\n",
    "    except:\n",
    "        df['geo'][i] = np.nan\n",
    "    try:\n",
    "        df['place'][i] = eval(df['place'][i])\n",
    "    except:\n",
    "        df['place'][i] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:23:49.913022Z",
     "start_time": "2021-04-18T15:23:49.890834Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:31:46.495459Z",
     "start_time": "2021-04-18T15:31:46.017199Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.distplot(df['retweet_count'], color='#05264c', rug=True, kde=True)\n",
    "plt.xlabel('retweet_count',fontsize=14)\n",
    "plt.ylabel('density',fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:31:34.958932Z",
     "start_time": "2021-04-18T15:31:34.823499Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(y=df['retweet_count'],color='#05264c')\n",
    "plt.ylabel('retweet_count',fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:51:00.445603Z",
     "start_time": "2021-04-18T15:51:00.099799Z"
    }
   },
   "outputs": [],
   "source": [
    "def bar_chart(col,num):\n",
    "    df[col].value_counts()[:num].plot.barh(color='#05264c', figsize=(10, 5))\n",
    "    plt.title(f'Top {num} {col}s')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel('count')\n",
    "    plt.ylabel(col)\n",
    "    plt.tight_layout()\n",
    "\n",
    "bar_chart('user_location',20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:40:24.870622Z",
     "start_time": "2021-04-18T15:40:23.806600Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "pairplot = sns.pairplot(df[['retweet_count','user_followers_count']], diag_kind='kde', palette='#05264c')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T16:06:02.706634Z",
     "start_time": "2021-04-18T16:06:02.483681Z"
    }
   },
   "outputs": [],
   "source": [
    "# number of tweets over time\n",
    "month_count = np.unique(df['created_at_month'],return_counts=True)\n",
    "months = month_count[0]\n",
    "plt.figure(figsize=(10, 5))\n",
    "bins = np.arange(1,13)\n",
    "plt.hist(df['created_at_month'], bins=bins, color='#05264c')\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('month')\n",
    "plt.xticks(bins)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Localization\n",
    "\n",
    "\n",
    "\n",
    "In this section, we look into getting a location for the tweets. This by 'geo', then 'place', then 'user_location'. For all, we try to get the location from the tweet text itself based on the TAGGS algorithm called geo-parsing  (https://link.springer.com/content/pdf/10.1007/s41651-017-0010-6.pdf) \n",
    "\n",
    "*Note that the “coordinates” attributes is formatted as [LONGITUDE, latitude], while the “geo” attribute is formatted as [latitude, LONGITUDE].\n",
    "\n",
    "- Coordinates\n",
    "- Place\n",
    "- User location\n",
    "- Geo-parsing\n",
    "\n",
    "Geo (coordinate) attribute \n",
    "\n",
    "- Only relevant tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T09:31:27.331992Z",
     "start_time": "2021-04-08T09:31:27.302318Z"
    }
   },
   "outputs": [],
   "source": [
    "# filter only relevant tweets\n",
    "geo_df = df[~df['geo'].isna()]\n",
    "geo_df = geo_df[geo_df['relevant']==1].reset_index(drop=True)\n",
    "geo_df['coords'] = [geo_df['geo'][i]['coordinates'] for i in range(len(geo_df))]\n",
    "\n",
    "print(len(geo_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T09:47:24.166607Z",
     "start_time": "2021-04-06T09:47:24.109372Z"
    }
   },
   "outputs": [],
   "source": [
    "geo_df['lat'] = [geo_df['coords'][i][0] for i in range(len(geo_df))]\n",
    "geo_df['lon'] = [geo_df['coords'][i][1] for i in range(len(geo_df))]\n",
    "#geo_df.to_csv('data/geo_tweets.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T09:42:18.061191Z",
     "start_time": "2021-04-06T09:42:17.431624Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "madrid = [40.416775, -3.703790]\n",
    "locationlist = list(geo_df['coords'])\n",
    "m = folium.Map(location=madrid, tiles='cartodbpositron', zoom_start=2)\n",
    "marker_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "for point in range(0, len(locationlist)):\n",
    "    folium.Marker(locationlist[point], popup=geo_df['full_text'][point]).add_to(marker_cluster)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place attribute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T12:42:12.415466Z",
     "start_time": "2021-03-25T12:42:12.303634Z"
    }
   },
   "outputs": [],
   "source": [
    "#users with places\n",
    "place_df = df_join[~df_join['place'].isna()]\n",
    "print(len(place_df))\n",
    "\n",
    "#places and NOT geo\n",
    "geo_users = list(geo_df['user_id'])\n",
    "place_df = place_df[~place_df['user_id'].isin(geo_users)].reset_index(drop=True)\n",
    "print(len(place_df))\n",
    "\n",
    "# add place_id\n",
    "place_df['place_id'] =[place_df['place'][i]['id'] for i in range(len(place_df))]\n",
    "\n",
    "# get dataframe with places metadata\n",
    "places = [place_df['place'][i] for i in range(len(place_df))]\n",
    "df_places= pd.DataFrame(places)\n",
    "\n",
    "df_places= df_places.rename(columns={'id':'place_id'})\n",
    "df_places = df_places.drop_duplicates(subset=['place_id'])\n",
    "\n",
    "# merge to get all place details\n",
    "place_df2 =pd.merge(place_df.drop(columns=['place']),df_places, on='place_id',how='inner')\n",
    "\n",
    "print(len(place_df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T12:42:12.948010Z",
     "start_time": "2021-03-25T12:42:12.923149Z"
    }
   },
   "outputs": [],
   "source": [
    "userloc_df = df_join[~df_join['user_location'].isna()]\n",
    "\n",
    "noloc_df = df_join[(df_join['user_location'].isna()) & (df_join['geo'].isna())  & (df_join['place'].isna()) ]\n",
    "noloc_df2 = df_join[(df_join['geo'].isna()) & (df_join['place'].isna()) ]\n",
    "len(noloc_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T12:42:13.489271Z",
     "start_time": "2021-03-25T12:42:13.484860Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'total : {len(df_join)}')\n",
    "print(30*'_' + '\\n')\n",
    "\n",
    "print(f'geotagged : {len(geo_df)}')\n",
    "print(f'places : {len(place_df2)}')\n",
    "print(f'user location : {len(userloc_df)}')\n",
    "print(f'no location : {len(noloc_df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAGGS algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T12:42:15.404959Z",
     "start_time": "2021-03-25T12:42:15.392486Z"
    }
   },
   "outputs": [],
   "source": [
    "dff= df_join[df_join['user_location'].isna()]\n",
    "len(dff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T12:42:16.008060Z",
     "start_time": "2021-03-25T12:42:16.001798Z"
    }
   },
   "outputs": [],
   "source": [
    "df_places['place_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T12:42:16.278515Z",
     "start_time": "2021-03-25T12:42:16.273980Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(noloc_df['full_text'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Text classification\n",
    "\n",
    "1. Basic NLP algorithms; tf-idf; logistic classifier, naive bayes, SVM\n",
    "2. Deep learning; word2vec; CNN\n",
    "3. Transfer learning; ULMFiT, GPT-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T14:46:38.549083Z",
     "start_time": "2021-04-15T14:46:38.534741Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# relevant variables\n",
    "df_rel = df[['full_text','tokens','relevant']]\n",
    "df_rel.head()                                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T14:46:59.383160Z",
     "start_time": "2021-04-15T14:46:58.878850Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df_rel['tokens']\n",
    "y = df_rel['relevant']\n",
    "\n",
    "print('Relevant:', sum(y), 'that is:' , round(sum(y)/len(y)*100,2), '%')\n",
    "print('Non-relevant:', len(y)-sum(y), 'that is:' ,round((len(y)-sum(y))/len(y)*100,2),'%')\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "p1 = plt.barh(1,100,color='lightgreen')\n",
    "p2 = plt.barh(1,(len(y)-sum(y))/len(y)*100,color='firebrick')\n",
    "\n",
    "plt.legend((p1[0], p2[0]), ('Relevant', 'Non-relevant'),loc='upper center')\n",
    "plt.yticks([1,1.8])\n",
    "plt.title('Class balance',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T14:47:29.622093Z",
     "start_time": "2021-04-15T14:47:29.593237Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_fit(X, y, model, clf_model):   \n",
    "    \n",
    "    X_c = model.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_c, y, random_state=0)\n",
    "    \n",
    "    print('* features: {}'.format(X_c.shape[1]))\n",
    "    print('* train records: {}'.format(X_train.shape[0]))\n",
    "    print('* test records: {}'.format(X_test.shape[0]))\n",
    "   \n",
    "    clf = clf_model.fit(X_train, y_train)\n",
    "    pred = clf_model.predict(X_test)\n",
    "    \n",
    "    conf_mat = confusion_matrix(y_test.tolist(), pred)\n",
    "    \n",
    "    print('\\nConfusion matrix')\n",
    "    print(conf_mat)\n",
    "    TN = conf_mat[0][0]\n",
    "    FP = conf_mat[0][1]\n",
    "    FN = conf_mat[1][0]\n",
    "    TP = conf_mat[1][1]\n",
    "\n",
    "    acc = (TP+TN)/(TP+TN+FP+FN)\n",
    "    prec = TP/(TP+FP)\n",
    "    rec = TP/(TP+FN)\n",
    "    print('Accuracy:',(acc))\n",
    "    print('Precision: ', prec)\n",
    "    print('Recall: ', rec)\n",
    "    \n",
    "    return clf, X_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: [https://www.kaggle.com/laowingkin/amazon-fine-food-review-sentiment-analysis](https://www.kaggle.com/laowingkin/amazon-fine-food-review-sentiment-analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T14:47:39.115030Z",
     "start_time": "2021-04-15T14:47:37.733746Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(preprocessor=' '.join, stop_words='english', lowercase=True)\n",
    "\n",
    "print('Dummy classifier')\n",
    "clf_base, X_c_base = text_fit(X, y, tfidf, DummyClassifier())\n",
    "print(60*'__')\n",
    "\n",
    "print('\\nLogistic regression')\n",
    "clf_log, X_c_log = text_fit(X, y, tfidf, LogisticRegression())\n",
    "print(60*'__')\n",
    "\n",
    "print('\\nNaive Bayes')\n",
    "clf_NB, X_c_NB = text_fit(X, y, tfidf, MultinomialNB())\n",
    "\n",
    "print(60*'__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T14:47:56.191236Z",
     "start_time": "2021-04-15T14:47:56.176116Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_words(model, clf_model, out):\n",
    "    w = model.get_feature_names()\n",
    "    coef = clf_model.coef_.tolist()[0]\n",
    "    coeff_df = pd.DataFrame({'Word': w, 'Coefficient': coef})\n",
    "    coeff_df = coeff_df.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
    "    if out == 1:\n",
    "        print('')\n",
    "        print('*Top 20 relevant*')\n",
    "        print(coeff_df.head(20).to_string(index=False))\n",
    "        print('')\n",
    "        print('*Top 20 non-relevant*')\n",
    "        print(coeff_df.tail(20).to_string(index=False))\n",
    "\n",
    "    return coeff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T14:47:58.041682Z",
     "start_time": "2021-04-15T14:47:57.974911Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Logistic regression:')\n",
    "\n",
    "_ = print_words(tfidf, clf_log, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN \n",
    "Source: https://towardsdatascience.com/cnn-sentiment-analysis-1d16b7c5a0e7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our problem is a binary classification. We need to pass our model a two-dimensional output vector. For that, we add two one hot encoded columns to our data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T14:49:18.511534Z",
     "start_time": "2021-04-15T14:49:18.484126Z"
    }
   },
   "outputs": [],
   "source": [
    "# relevant variables\n",
    "df_rel = df[['full_text','tokens','relevant']] \n",
    "data = df_rel.rename(columns={'relevant': 'label'}, inplace=False)\n",
    "rel = []\n",
    "notrel = []\n",
    "for l in data.label:\n",
    "    if l == 0:\n",
    "        rel.append(0)\n",
    "        notrel.append(1)\n",
    "    elif l == 1:\n",
    "        rel.append(1)\n",
    "        notrel.append(0)\n",
    "data['relevant'] = rel\n",
    "data['not_relevant'] = notrel\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T14:49:28.018455Z",
     "start_time": "2021-04-15T14:49:27.910412Z"
    }
   },
   "outputs": [],
   "source": [
    "# splitting data into test and train\n",
    "data_train, data_test = train_test_split(data, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T14:49:30.885020Z",
     "start_time": "2021-04-15T14:49:30.721521Z"
    }
   },
   "outputs": [],
   "source": [
    "# build training vocabulary\n",
    "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T14:49:32.324775Z",
     "start_time": "2021-04-15T14:49:32.303872Z"
    }
   },
   "outputs": [],
   "source": [
    "# build testing vocabulary \n",
    "all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-15T14:50:21.335Z"
    }
   },
   "outputs": [],
   "source": [
    "# load google news Word2Vec model \n",
    "word2vec_path = 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T08:10:45.239547Z",
     "start_time": "2021-04-13T08:10:45.219066Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
    "    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, generate_missing=generate_missing))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T08:10:48.105212Z",
     "start_time": "2021-04-13T08:10:45.242357Z"
    }
   },
   "outputs": [],
   "source": [
    "# get embeddings\n",
    "training_embeddings = get_word2vec_embeddings(word2vec, data_train, generate_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T08:10:48.114987Z",
     "start_time": "2021-04-13T08:10:48.108436Z"
    }
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T08:10:48.706743Z",
     "start_time": "2021-04-13T08:10:48.118057Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenize and pad sequences\n",
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data_train['full_text'].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(data_train['full_text'].tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))\n",
    "\n",
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T08:10:49.222501Z",
     "start_time": "2021-04-13T08:10:48.708904Z"
    }
   },
   "outputs": [],
   "source": [
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T08:10:49.258896Z",
     "start_time": "2021-04-13T08:10:49.224149Z"
    }
   },
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(data_test[\"full_text\"].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T08:10:49.271180Z",
     "start_time": "2021-04-13T08:10:49.263182Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining CNN\n",
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    " \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embeddings],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    \n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "    \n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=200, \n",
    "                        kernel_size=filter_size, \n",
    "                        activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "    \n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "    \n",
    "    x = Dropout(0.1)(l_merge)  \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T08:10:49.297200Z",
     "start_time": "2021-04-13T08:10:49.273318Z"
    }
   },
   "outputs": [],
   "source": [
    "label_names = ['relevant','notrelevant']\n",
    "\n",
    "x_train = train_cnn_data\n",
    "y_train = data_train[label_names].values\n",
    "\n",
    "x_test = test_cnn_data\n",
    "y_test = data_test[label_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T08:10:49.679698Z",
     "start_time": "2021-04-13T08:10:49.299356Z"
    }
   },
   "outputs": [],
   "source": [
    "model = ConvNet(train_embedding_weights, \n",
    "                MAX_SEQUENCE_LENGTH, \n",
    "                len(train_word_index)+1, \n",
    "                EMBEDDING_DIM, \n",
    "                len(list(label_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T08:11:38.801624Z",
     "start_time": "2021-04-13T08:10:49.682522Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training CNN\n",
    "num_epochs = 3\n",
    "batch_size = 32\n",
    "\n",
    "clear_session()\n",
    "\n",
    "hist = model.fit(x_train, \n",
    "                 y_train, \n",
    "                 epochs=num_epochs, \n",
    "                 validation_split=0.1, \n",
    "                 shuffle=True, \n",
    "                 batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T08:11:39.655936Z",
     "start_time": "2021-04-13T08:11:38.805293Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)\n",
    "labels = [1, 0]\n",
    "\n",
    "prediction_labels = []\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])\n",
    "\n",
    "sum(data_test.label == prediction_labels)/len(prediction_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T08:11:39.671295Z",
     "start_time": "2021-04-13T08:11:39.658743Z"
    }
   },
   "outputs": [],
   "source": [
    "data_test.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T08:25:21.033948Z",
     "start_time": "2021-04-13T08:25:15.124979Z"
    }
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=False)\n",
    "print(\"Training accuracy: {:.4f}\".format(accuracy))\n",
    "\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=False)\n",
    "print(\"Testing accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T08:25:21.556751Z",
     "start_time": "2021-04-13T08:25:21.037225Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "acc = hist.history['acc']\n",
    "val_acc = hist.history['val_acc']\n",
    "loss = hist.history['loss']\n",
    "val_loss = hist.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "x = range(1, len(acc) + 1)\n",
    "plt.plot(x, acc, 'b', label='Training acc')\n",
    "plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "plt.xticks(x)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, loss, 'b', label='Training loss')\n",
    "plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "plt.xticks(x)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T08:25:23.736013Z",
     "start_time": "2021-04-13T08:25:23.629803Z"
    }
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_class)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T10:05:52.730892Z",
     "start_time": "2021-04-24T10:05:52.624382Z"
    }
   },
   "outputs": [],
   "source": [
    "# filter only relevant tweets\n",
    "geo_df = df[~df['geo'].isna()].reset_index(drop=True)\n",
    "geo_df = geo_df[geo_df['relevant']==1].reset_index(drop=True)\n",
    "for i in range(len(geo_df)):\n",
    "    try: \n",
    "        geo_df['geo'][i] = eval(geo_df['geo'][i])\n",
    "    except:\n",
    "        geo_df['geo'][i] = geo_df['geo'][i]\n",
    "\n",
    "#geo_df['coords'] = [geo_df['geo'][i]['coordinates'] for i in range(len(geo_df))]\n",
    "geo_df['lat'] = [geo_df['geo'][i]['coordinates'][0] for i in range(len(geo_df))]\n",
    "geo_df['lon'] = [geo_df['geo'][i]['coordinates'][1] for i in range(len(geo_df))]\n",
    "\n",
    "#geo_df.to_csv('data/geo_tweets.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folium, Leaflet, OpenStreetMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T16:27:44.093140Z",
     "start_time": "2021-04-18T16:27:43.223630Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# base map\n",
    "m = folium.Map([20.416775, -3.70379], tiles=None, zoom_start=2)\n",
    "\n",
    "# tile layers\n",
    "folium.TileLayer('cartodbpositron', show=True, name=\"light\").add_to(m)\n",
    "folium.TileLayer('cartodbdark_matter', show=False, name=\"dark\").add_to(m)\n",
    "folium.TileLayer('openstreetmap', show=False, name=\"color\").add_to(m)\n",
    "\n",
    "# add location marker cluster\n",
    "mc = MarkerCluster(name='Tweets').add_to(m)\n",
    "\n",
    "# create marker at locations\n",
    "for lat, lon, user_location, full_text, created_at, retweet_count in zip(geo_df['lat'], geo_df['lon'], geo_df['user_location'], \n",
    "                                     geo_df['full_text'], geo_df['created_at'], geo_df['retweet_count']):\n",
    "    text = folium.Html('Tweet: {}<br> User location: {}<br> Created at: {}<br> Retweet count: {}<br>'.format(full_text, user_location, created_at, retweet_count), script=True)\n",
    "    popup = folium.Popup(text, max_width=300)\n",
    "    folium.CircleMarker(location = [lat, lon],\n",
    "                        radius = 2,\n",
    "                        weight = 5,\n",
    "                        color = '#081d58',\n",
    "                        fill_color = '#081d58',\n",
    "                        fill = True,\n",
    "                        popup = popup,\n",
    "                        tooltip = 'Click on Tweet'\n",
    "                        ).add_to(mc)\n",
    "mc.add_to(m)\n",
    "\n",
    "# add layer control\n",
    "folium.LayerControl('topright', collapsed=True).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kepler.gl\n",
    "\n",
    "https://medium.com/nightingale/how-to-create-eye-catching-maps-with-python-and-kepler-gl-e7e897eff8ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T12:10:52.502234Z",
     "start_time": "2021-04-24T12:10:52.439409Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "viz_cols = ['lon','lat','created_at','full_text','user_location']\n",
    "kepler_map = keplergl.KeplerGl(height=500)\n",
    "kepler_map.add_data(data=geo_df[viz_cols], name=\"Extreme weather events\")\n",
    "#kepler_map.save_to_html(file_name=\"kepler_map.html\")\n",
    "kepler_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T16:34:46.460910Z",
     "start_time": "2021-04-18T16:34:46.458398Z"
    }
   },
   "outputs": [],
   "source": [
    "#config = kepler_map.config\n",
    "#config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotly, Mapbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T10:06:54.274882Z",
     "start_time": "2021-04-24T10:06:54.271946Z"
    }
   },
   "outputs": [],
   "source": [
    "mapbox_token = 'pk.eyJ1IjoiczE1Mzc0OCIsImEiOiJja25wcDlwdjYxcWJmMnFueDhhbHdreTlmIn0.DXfj5S2H91AZEPG1JnHbxg'\n",
    "px.set_mapbox_access_token(mapbox_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T10:06:54.820928Z",
     "start_time": "2021-04-24T10:06:54.744093Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# point map\n",
    "scatter_map = px.scatter_mapbox(geo_df, lat=\"lat\", lon=\"lon\", hover_name=\"full_text\", \n",
    "                        hover_data=[\"created_at\", \"user_location\",'retweeted'],\n",
    "                        color_discrete_sequence=[\"teal\"], \n",
    "                        zoom=1, height=500)\n",
    "scatter_map.update_layout(mapbox_style=\"carto-positron\",margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "scatter_map.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T11:01:45.233490Z",
     "start_time": "2021-04-24T11:01:45.163478Z"
    }
   },
   "outputs": [],
   "source": [
    "scatter_map = px.scatter_mapbox(\n",
    "    geo_df, lat=\"lat\", lon=\"lon\", \n",
    "    hover_data=['full_text',\"created_at\",\"user_location\"],\n",
    "    color = 'retweet_count',\n",
    "    color_continuous_scale='teal',\n",
    "    zoom=1, height=500)\n",
    "scatter_map.update_layout(mapbox_style=\"dark\",\n",
    "                          margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "scatter_map.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T10:06:59.078504Z",
     "start_time": "2021-04-24T10:06:58.996026Z"
    }
   },
   "outputs": [],
   "source": [
    "# size by retweet_count\n",
    "scatter_map = px.scatter_mapbox(\n",
    "    geo_df, lat=\"lat\", lon=\"lon\", \n",
    "    size = 'retweet_count',\n",
    "    size_max = 15,\n",
    "    color='retweeted',\n",
    "    hover_data=['full_text'],\n",
    "    #color_discrete_sequence=[\"teal\",\"\"],\n",
    "    zoom=1, height=500)\n",
    "scatter_map.update_layout(mapbox_style=\"dark\",\n",
    "                          margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "scatter_map.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T15:00:35.433908Z",
     "start_time": "2021-04-24T15:00:35.407014Z"
    }
   },
   "outputs": [],
   "source": [
    "# color by retweet_count\n",
    "colorscales = px.colors.named_colorscales()\n",
    "#mapbox_token = open(\".mapbox_token\").read()\n",
    "\n",
    "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "app = JupyterDash(__name__, external_stylesheets=external_stylesheets,\n",
    "                  meta_tags=[{\"name\": \"viewport\", \"content\": \"width=device-width, initial-scale=1\"}])\n",
    "\n",
    "server = app.server\n",
    "#app.title = tabtitle\n",
    "app.config.suppress_callback_exceptions = True\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.P(\"Color Scale\"),\n",
    "    dcc.Dropdown(id='colorscale',\n",
    "                 options=[{\"value\": x, \"label\": x} for x in colorscales],\n",
    "                 value='teal'),\n",
    "    dcc.Graph(id=\"scatter_map\"),\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"scatter_map\", \"figure\"),\n",
    "    [Input(\"colorscale\", \"value\")])\n",
    "def change_colorscale(scale):\n",
    "    fig = px.scatter_mapbox(geo_df, lat=\"lat\", lon=\"lon\",\n",
    "                            color='retweet_count',\n",
    "                            hover_data=[\"full_text\", \"user_location\"],\n",
    "                            color_continuous_scale=scale,\n",
    "                            zoom=1, height=500)\n",
    "    fig.update_layout(mapbox_style=\"dark\",\n",
    "                      margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0},\n",
    "                      hovermode='closest',\n",
    "                      mapbox=dict(accesstoken=mapbox_token,\n",
    "                                  bearing=0,\n",
    "                                  pitch=0))\n",
    "    return fig\n",
    "\n",
    "app.run_server(mode='external', port=8060, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T10:07:50.620070Z",
     "start_time": "2021-04-24T10:07:50.511192Z"
    }
   },
   "outputs": [],
   "source": [
    "hexabin_map = ff.create_hexbin_mapbox(data_frame=geo_df[['lat','lon']], lat=\"lat\", lon=\"lon\",\n",
    "                                      nx_hexagon=25, opacity=0.5, labels={\"color\": \"Relevant Tweets\"},\n",
    "                                      min_count=1, color_continuous_scale=\"Teal\",\n",
    "                                      show_original_data=True, height=500, zoom=0.95,\n",
    "                                      original_data_marker=dict(size=5, opacity=0.7, color=\"Teal\")\n",
    ")\n",
    "hexabin_map.update_layout(mapbox_style=\"carto-positron\",\n",
    "                          margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "hexabin_map.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T10:08:10.633292Z",
     "start_time": "2021-04-24T10:08:10.608593Z"
    }
   },
   "outputs": [],
   "source": [
    "# find number of tweets by date\n",
    "df['Date'] = pd.to_datetime(df['created_at']).dt.date\n",
    "count_dates = df.groupby('Date').size().values\n",
    "time_df = df.drop_duplicates(subset=\"Date\").assign(Count=count_dates)\n",
    "time_df = time_df[['Date','Count']].sort_values(by='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T10:08:14.481162Z",
     "start_time": "2021-04-24T10:08:14.392364Z"
    }
   },
   "outputs": [],
   "source": [
    "line_fig = px.line(time_df, x='Date', y='Count', title='Relevant Tweets over time')\n",
    "\n",
    "line_fig.update_xaxes(\n",
    "    rangeslider_visible=True,\n",
    "    rangeselector=dict(\n",
    "        buttons=list([\n",
    "            dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n",
    "            #dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n",
    "            #dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n",
    "            dict(step=\"all\")\n",
    "        ])\n",
    "    )\n",
    ")\n",
    "line_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T07:29:27.277376Z",
     "start_time": "2021-04-20T07:29:27.222635Z"
    }
   },
   "outputs": [],
   "source": [
    "app = JupyterDash(external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
    "app.layout = html.Div([\n",
    "    dcc.Graph(figure=fig)\n",
    "])\n",
    "app.run_server(mode='external', port=8060, use_reloader=False) # debug=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T20:31:51.703775Z",
     "start_time": "2021-04-23T20:31:51.682884Z"
    }
   },
   "outputs": [],
   "source": [
    "app._terminate_server_for_port(\"localhost\", 8060)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T19:45:53.192093Z",
     "start_time": "2021-04-23T19:45:53.185074Z"
    }
   },
   "outputs": [],
   "source": [
    "{int(month): f'{calendar.month_name[int(month)][:3]} {str(year)[:4]}' for \n",
    "     year, month in zip(geo_df['created_at_year'], np.arange(1, 13))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T13:52:44.655260Z",
     "start_time": "2021-04-24T13:52:44.624946Z"
    }
   },
   "outputs": [],
   "source": [
    "area_select = ['Alberta','Queensland']\n",
    "geo_data = geo_df\n",
    "geo_data[geo_data[\"area\"].isin(area_select)].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T14:04:42.903574Z",
     "start_time": "2021-04-24T14:04:42.892428Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered_data = geo_data[geo_data[\"area\"].isin(area_select)]\n",
    "filtered_data[\"lat\"].tolist()\n",
    "retweeted = filtered_data[\"retweeted\"].tolist()\n",
    "retweeted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-24T16:45:26.069Z"
    }
   },
   "outputs": [],
   "source": [
    "areas = geo_df[\"area\"].unique()\n",
    "options = [{\"label\": i, \"value\": i} for i in areas]\n",
    "print([eval(areas[i]) for i in range(len(areas))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T09:31:06.598713Z",
     "start_time": "2021-04-25T09:31:04.197708Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import dash_bootstrap_components as dbc\n",
    "from dash.dependencies import Input, Output, State #ClientsideFunction\n",
    "from dash.exceptions import PreventUpdate\n",
    "from jupyter_dash import JupyterDash # only in nb\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import ast \n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T09:31:32.430783Z",
     "start_time": "2021-04-25T09:31:30.022133Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisestyve/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash app running on http://127.0.0.1:8060/\n"
     ]
    }
   ],
   "source": [
    "# Initiate app\n",
    "# app = dash.Dash()\n",
    "app = JupyterDash(\n",
    "    __name__,\n",
    "    meta_tags=[{\n",
    "            \"name\": \"viewport\",\n",
    "            \"content\": \"width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no\",\n",
    "    }],\n",
    ")\n",
    "server = app.server\n",
    "app.config.suppress_callback_exceptions = True\n",
    "\n",
    "#external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "githublink='https://github.com/s153748/extreme-weather-detection'\n",
    "mapbox_access_token = 'pk.eyJ1IjoiczE1Mzc0OCIsImEiOiJja25wcDlwdjYxcWJmMnFueDhhbHdreTlmIn0.DXfj5S2H91AZEPG1JnHbxg'\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('data/final_labelled_tweets.csv')\n",
    "\n",
    "# Data prep\n",
    "df.dropna(subset=['tokens'], inplace=True)\n",
    "df['tokens'] = [literal_eval(s) for s in df['tokens']]\n",
    "for i in range(len(df)):\n",
    "    try:\n",
    "        df['geo'][i] = eval(df['geo'][i])\n",
    "    except:\n",
    "        df['geo'][i] = np.nan\n",
    "    try:\n",
    "        df['place'][i] = eval(df['place'][i])\n",
    "    except:\n",
    "        df['place'][i] = np.nan\n",
    "    \n",
    "geo_df = df[~df['geo'].isna()].reset_index(drop=True)\n",
    "geo_df = geo_df[geo_df['relevant'] == 1].reset_index(drop=True)\n",
    "for i in range(len(geo_df)):\n",
    "    try:\n",
    "        geo_df['geo'][i] = eval(geo_df['geo'][i])\n",
    "    except:\n",
    "        geo_df['geo'][i] = geo_df['geo'][i]\n",
    "\n",
    "# Get coordinates\n",
    "geo_df['lat'] = [geo_df['geo'][i]['coordinates'][0] for i in range(len(geo_df))]\n",
    "geo_df['lon'] = [geo_df['geo'][i]['coordinates'][1] for i in range(len(geo_df))]\n",
    "\n",
    "# Find number of tweets by date\n",
    "df['Date'] = pd.to_datetime(df['created_at']).dt.date\n",
    "count_dates = df.groupby('Date').size().values\n",
    "time_df = df.drop_duplicates(subset=\"Date\").assign(Count=count_dates).sort_values(by='Date')\n",
    "\n",
    "# Set graph options\n",
    "graph_list = ['Point map','Hexagon map']\n",
    "area_list = geo_df[\"area\"].unique()\n",
    "\n",
    "def build_upper_left_panel():\n",
    "    return html.Div(\n",
    "        id=\"upper-left\",\n",
    "        className=\"four columns\", \n",
    "        children=[\n",
    "            html.P(\n",
    "                className=\"section-title\",\n",
    "                children=\"Choose graph type or specific areas to inspect for the lists below\",\n",
    "            ),\n",
    "            html.Div(\n",
    "                className=\"control-row-1\",\n",
    "                children=[\n",
    "                    html.Div(\n",
    "                        id=\"graph-select-outer\",\n",
    "                        children=[\n",
    "                            html.Label(\"Select Graph Type\"),\n",
    "                            dcc.Dropdown(\n",
    "                                id=\"graph-select\",\n",
    "                                options=[{\"label\": i, \"value\": i} for i in graph_list],\n",
    "                                value=graph_list[0],\n",
    "                            ),\n",
    "                        ],\n",
    "                    ),\n",
    "                ],\n",
    "            ),\n",
    "            html.Div(\n",
    "                id=\"area-select-outer\",\n",
    "                className=\"control-row-2\",\n",
    "                children=[\n",
    "                    html.Label(\"Select Area\"),\n",
    "                    html.Div(\n",
    "                        id=\"checklist-container\",\n",
    "                        children=dcc.Checklist(\n",
    "                            id=\"area-select-all\",\n",
    "                            options=[{\"label\": \"Select All Areas\", \"value\": \"All\"}],\n",
    "                            value=[],\n",
    "                        ),\n",
    "                    ),\n",
    "                    html.Div(\n",
    "                        id=\"area-select-dropdown-outer\",\n",
    "                        children=dcc.Dropdown(\n",
    "                            id=\"area-select\", multi=True, searchable=True,\n",
    "                        ),\n",
    "                    ),\n",
    "                ],\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "def generate_geo_map(geo_data, month_select, graph_select, area_select):\n",
    "    \n",
    "    month_filtered = geo_data[geo_data.created_at_month == month_select]\n",
    "    filtered_data = month_filtered[month_filtered[\"area\"].isin(area_select)]\n",
    "    \n",
    "    if graph_select == 'Point map':\n",
    "        fig = px.scatter_mapbox(filtered_data, \n",
    "                                lat=\"lat\", \n",
    "                                lon=\"lon\",\n",
    "                                color='retweet_count',\n",
    "                                size='retweet_count',\n",
    "                                size_max=15,\n",
    "                                height=500,\n",
    "                                width=800,\n",
    "                                hover_data=[\"full_text\"],\n",
    "                                color_continuous_scale='teal')\n",
    "    else:\n",
    "        fig = ff.create_hexbin_mapbox(data_frame=filtered_data, \n",
    "                                      lat=\"lat\", \n",
    "                                      lon=\"lon\",\n",
    "                                      nx_hexagon=25, \n",
    "                                      opacity=0.5, \n",
    "                                      labels={\"color\": \"Relevant Tweets\"},\n",
    "                                      min_count=1, \n",
    "                                      color_continuous_scale=\"teal\",\n",
    "                                      show_original_data=True, \n",
    "                                      height=500,\n",
    "                                      width=800,\n",
    "                                      original_data_marker=dict(size=5, opacity=0.7, color=\"teal\")\n",
    "        )\n",
    "        \n",
    "    fig.update_layout(\n",
    "        margin=dict(l=10, r=10, t=20, b=10, pad=5),\n",
    "        plot_bgcolor=\"#171b26\",\n",
    "        paper_bgcolor=\"#171b26\",\n",
    "        clickmode=\"event+select\",\n",
    "        hovermode=\"closest\",\n",
    "        showlegend=False,\n",
    "        mapbox=go.layout.Mapbox(\n",
    "            accesstoken=mapbox_access_token,\n",
    "            bearing=10,\n",
    "            center=go.layout.mapbox.Center(\n",
    "                lat=filtered_data.lat.mean(), lon=filtered_data.lon.mean()\n",
    "            ),\n",
    "            pitch=5,\n",
    "            zoom=2,\n",
    "            style=\"mapbox://styles/plotlymapbox/cjvppq1jl1ips1co3j12b9hex\",\n",
    "        )\n",
    "    )\n",
    "        \n",
    "    return fig\n",
    "\n",
    "def generate_line_chart(time_data):\n",
    "    fig = px.line(time_data,\n",
    "                  x='Date',\n",
    "                  y='Count',\n",
    "                  title='Relevant Tweets Over Time')\n",
    "    fig.update_xaxes(\n",
    "        rangeslider_visible=True,\n",
    "        rangeselector=dict(buttons=list([\n",
    "            dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n",
    "            dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n",
    "            dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n",
    "            dict(step=\"all\")\n",
    "        ])))\n",
    "    fig.update_layout(\n",
    "        plot_bgcolor=\"#171b26\",\n",
    "        paper_bgcolor=\"#171b26\",\n",
    "        showlegend=False\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "# Set up the layout\n",
    "app.layout = html.Div(\n",
    "    className=\"container scalable\",\n",
    "    children=[\n",
    "        html.Div(\n",
    "            id=\"banner\",\n",
    "            className=\"banner\",\n",
    "            children=[\n",
    "                html.H6(\"Extreme Weather Event Detection\"),\n",
    "                #html.Img(src=app.get_asset_url(\"plotly_logo_white.png\")),\n",
    "                html.A('View on Github', href=githublink),\n",
    "            ],\n",
    "        ),\n",
    "        html.Div(\n",
    "            id=\"upper-container\",\n",
    "            className=\"row\",\n",
    "            children=[\n",
    "                build_upper_left_panel(),\n",
    "                html.Div(\n",
    "                    id=\"geo-map-outer\",\n",
    "                    className=\"four columns\",\n",
    "                    children=[\n",
    "                        html.P(\n",
    "                            id=\"map-title\",\n",
    "                            children=\"Spatial Development of Relevant Tweets\"\n",
    "                        ),\n",
    "                        html.Div(\n",
    "                            id=\"geo-map-loading-outer\",\n",
    "                            children=[\n",
    "                                dcc.Loading(\n",
    "                                    id=\"loading\",\n",
    "                                    children=[\n",
    "                                        dcc.Graph(\n",
    "                                            id=\"geo-map\",\n",
    "                                            figure={\n",
    "                                                \"data\": [],\n",
    "                                                \"layout\": dict(\n",
    "                                                    plot_bgcolor=\"#171b26\",\n",
    "                                                    paper_bgcolor=\"#171b26\",\n",
    "                                                ),\n",
    "                                            },\n",
    "                                        ),\n",
    "                                        dcc.Slider(\n",
    "                                            id='month-slider',\n",
    "                                            min=geo_df['created_at_month'].min(),\n",
    "                                            max=geo_df['created_at_month'].max(),\n",
    "                                            value=geo_df['created_at_month'].min(),\n",
    "                                            marks={int(month): f'{calendar.month_name[int(month)][:3]} {str(year)[:4]}' for year, month in zip(\n",
    "                                                geo_df['created_at_year'], geo_df['created_at_month'])},\n",
    "                                            step=None\n",
    "                                        ),\n",
    "                                    ]\n",
    "                                )\n",
    "                            ],\n",
    "                        ),\n",
    "                    ],\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "       html.Div(\n",
    "            id=\"lower-container\",\n",
    "            className=\"row\",\n",
    "            children=[\n",
    "                html.Div(\n",
    "                    id=\"line-chart-outer\",\n",
    "                    className=\"four columns\",\n",
    "                    children=[\n",
    "                        html.P(\n",
    "                            id=\"line-chart-title\",\n",
    "                            children=\"Temporal Development of Relevant Tweets\"\n",
    "                        ),\n",
    "                        html.Div(\n",
    "                            id=\"line-chart-loading-outer\",\n",
    "                            children=[\n",
    "                                dcc.Loading(\n",
    "                                    id=\"loading-line-chart\",\n",
    "                                    children=[\n",
    "                                        dcc.Graph(\n",
    "                                            id=\"line-chart\",\n",
    "                                            figure=generate_line_chart(time_df)\n",
    "                                        )\n",
    "                                    ]\n",
    "                                )\n",
    "                            ]\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "@app.callback(\n",
    "    [\n",
    "        Output(\"area-select\", \"value\"),\n",
    "        Output(\"area-select\", \"options\"),\n",
    "        Output(\"map-title\", \"children\"),\n",
    "    ],\n",
    "    [Input(\"area-select-all\", \"value\"), Input(\"area-select\", \"value\"),],\n",
    ")\n",
    "def update_area_dropdown(select_all, area_select):\n",
    "    areas = geo_df[\"area\"].unique()\n",
    "    options = [{\"label\": i, \"value\": i} for i in areas]\n",
    "\n",
    "    ctx = dash.callback_context\n",
    "    if ctx.triggered[0][\"prop_id\"].split(\".\")[0] == \"area-select-all\":\n",
    "        if select_all == [\"All\"]:\n",
    "            value = [i[\"value\"] for i in options]\n",
    "        else:\n",
    "            value = dash.no_update\n",
    "    else:\n",
    "        value = areas[:4]\n",
    "    return (\n",
    "        value,\n",
    "        options,\n",
    "        \"Relevant Tweets in {}\".format(', '.join([str(area) for area in area_select if len(area_select)>0]))\n",
    "    )\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"checklist-container\", \"children\"),\n",
    "    [Input(\"area-select\", \"value\")],\n",
    "    [State(\"area-select\", \"options\"), State(\"area-select-all\", \"value\")],\n",
    ")\n",
    "def update_checklist(selected, select_options, checked):\n",
    "    if len(selected) < len(select_options) and len(checked) == 0:\n",
    "        raise PreventUpdate()\n",
    "    elif len(selected) < len(select_options) and len(checked) == 1:\n",
    "        return dcc.Checklist(\n",
    "            id=\"area-select-all\",\n",
    "            options=[{\"label\": \"Select All Areas\", \"value\": \"All\"}],\n",
    "            value=[],\n",
    "        )\n",
    "    elif len(selected) == len(select_options) and len(checked) == 1:\n",
    "        raise PreventUpdate()\n",
    "    return dcc.Checklist(\n",
    "        id=\"area-select-all\",\n",
    "        options=[{\"label\": \"Select All Areas\", \"value\": \"All\"}],\n",
    "        value=[\"All\"],\n",
    "    )\n",
    "\n",
    "@app.callback(\n",
    "    Output('geo-map', 'figure'),\n",
    "    [\n",
    "        Input('month-slider', 'value'),\n",
    "        Input(\"graph-select\", \"value\"),\n",
    "        Input(\"area-select\", \"value\"),\n",
    "    ],\n",
    ")\n",
    "def update_geo_map(month_select, graph_select, area_select):\n",
    "    \n",
    "    return generate_geo_map(geo_df, month_select, graph_select, area_select)\n",
    "\n",
    "app.run_server(mode='external', port=8060, use_reloader=False)\n",
    "#app.run_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T09:35:09.947998Z",
     "start_time": "2021-04-25T09:35:09.941802Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Relevant Tweets in Alberta'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Relevant Tweets in {}\".format(', '.join([str(area) for area in area_select if len(area_select)>0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T09:35:07.319566Z",
     "start_time": "2021-04-25T09:35:07.316465Z"
    }
   },
   "outputs": [],
   "source": [
    "area_select = ['Alberta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"AI for Climate Adaptation (AI4CA)\"\n",
    "\"Identification and Exploration of Extreme Weather Events From Twitter Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T10:27:36.126016Z",
     "start_time": "2021-04-22T10:27:36.123941Z"
    }
   },
   "outputs": [],
   "source": [
    "controls = dbc.FormGroup(\n",
    "    [\n",
    "        html.P('Dropdown', style={\n",
    "            'textAlign': 'center'\n",
    "        }),\n",
    "        dcc.Dropdown(\n",
    "            id='dropdown',\n",
    "            options=[{\n",
    "                'label': 'Value One',\n",
    "                'value': 'value1'\n",
    "            }, {\n",
    "                'label': 'Value Two',\n",
    "                'value': 'value2'\n",
    "            },\n",
    "                {\n",
    "                    'label': 'Value Three',\n",
    "                    'value': 'value3'\n",
    "                }\n",
    "            ],\n",
    "            value=['value1'],  # default value\n",
    "            multi=True\n",
    "        ),\n",
    "        html.Br(),\n",
    "        html.P('Check Box', style={\n",
    "            'textAlign': 'center'\n",
    "        }),\n",
    "        dbc.Card([dbc.Checklist(\n",
    "            id='check_list',\n",
    "            options=[{\n",
    "                'label': 'Value One',\n",
    "                'value': 'value1'\n",
    "            },\n",
    "                {\n",
    "                    'label': 'Value Two',\n",
    "                    'value': 'value2'\n",
    "                },\n",
    "                {\n",
    "                    'label': 'Value Three',\n",
    "                    'value': 'value3'\n",
    "                }\n",
    "            ],\n",
    "            value=['value1', 'value2'],\n",
    "            inline=True\n",
    "        )]),\n",
    "        html.Br(),\n",
    "        html.P('Radio Items', style={\n",
    "            'textAlign': 'center'\n",
    "        }),\n",
    "        dbc.Card([dbc.RadioItems(\n",
    "            id='radio_items',\n",
    "            options=[{\n",
    "                'label': 'Value One',\n",
    "                'value': 'value1'\n",
    "            },\n",
    "                {\n",
    "                    'label': 'Value Two',\n",
    "                    'value': 'value2'\n",
    "                },\n",
    "                {\n",
    "                    'label': 'Value Three',\n",
    "                    'value': 'value3'\n",
    "                }\n",
    "            ],\n",
    "            value='value1',\n",
    "            style={\n",
    "                'margin': 'auto'\n",
    "            }\n",
    "        )]),\n",
    "        html.Br(),\n",
    "        dbc.Button(\n",
    "            id='submit_button',\n",
    "            n_clicks=0,\n",
    "            children='Submit',\n",
    "            color='primary',\n",
    "            block=True\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Sidebar\n",
    "sidebar = html.Div(\n",
    "    [\n",
    "        html.H2('Config', style=TEXT_STYLE),\n",
    "        html.Hr(),\n",
    "        controls\n",
    "    ],\n",
    "    style=SIDEBAR_STYLE,\n",
    ")\n",
    "\n",
    "# Content\n",
    "content = html.Div(\n",
    "    [\n",
    "        html.H2('Extreme Weather Events Dashboard', style=TEXT_STYLE),\n",
    "        html.Hr(),\n",
    "        content_first_row\n",
    "        content_second_row,\n",
    "        content_third_row\n",
    "    ],\n",
    "    style=CONTENT_STYLE\n",
    ")\n",
    "\n",
    "content_first_row = dbc.Row([\n",
    "    dbc.Col(\n",
    "        dbc.Card(\n",
    "            [\n",
    "\n",
    "                dbc.CardBody(\n",
    "                    [\n",
    "                        html.H4(id='card_title_1', children=['Card Title 1'], className='card-title',\n",
    "                                style=CARD_TEXT_STYLE),\n",
    "                        html.P(id='card_text_1', children=['Sample text.'], style=CARD_TEXT_STYLE),\n",
    "                    ]\n",
    "                )\n",
    "            ]\n",
    "        ),\n",
    "        md=3\n",
    "    ),\n",
    "    dbc.Col(\n",
    "        dbc.Card(\n",
    "            [\n",
    "\n",
    "                dbc.CardBody(\n",
    "                    [\n",
    "                        html.H4('Card Title 2', className='card-title', style=CARD_TEXT_STYLE),\n",
    "                        html.P('Sample text.', style=CARD_TEXT_STYLE),\n",
    "                    ]\n",
    "                ),\n",
    "            ]\n",
    "\n",
    "        ),\n",
    "        md=3\n",
    "    ),\n",
    "    dbc.Col(\n",
    "        dbc.Card(\n",
    "            [\n",
    "                dbc.CardBody(\n",
    "                    [\n",
    "                        html.H4('Card Title 3', className='card-title', style=CARD_TEXT_STYLE),\n",
    "                        html.P('Sample text.', style=CARD_TEXT_STYLE),\n",
    "                    ]\n",
    "                ),\n",
    "            ]\n",
    "\n",
    "        ),\n",
    "        md=3\n",
    "    ),\n",
    "    dbc.Col(\n",
    "        dbc.Card(\n",
    "            [\n",
    "                dbc.CardBody(\n",
    "                    [\n",
    "                        html.H4('Card Title 4', className='card-title', style=CARD_TEXT_STYLE),\n",
    "                        html.P('Sample text.', style=CARD_TEXT_STYLE),\n",
    "                    ]\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "        md=3\n",
    "    )\n",
    "])\n",
    "\n",
    "content_second_row = dbc.Row(\n",
    "    [\n",
    "        dbc.Col(\n",
    "            dcc.Graph(id='scatter_map'), md=12,\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "content_third_row = dbc.Row(\n",
    "    [\n",
    "        dbc.Col(\n",
    "            dcc.Graph(id='line_map'), md=6\n",
    "        ),\n",
    "        dbc.Col(\n",
    "            dcc.Graph(id='graph'), md=6\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors\n",
    "colors = {\n",
    "    'background': '#262B3D',\n",
    "    'text': '#FFF'\n",
    "}\n",
    "\n",
    "# Description\n",
    "def description_card():\n",
    "    return html.Div(\n",
    "        id=\"description-card\",\n",
    "        children=[html.H3(children=\"Extreme Weather Event Detection\", style={'color': colors['text']}),\n",
    "                  html.Div(id=\"intro\", style={'color': colors['text']},\n",
    "                           children=\"Explore the Tweets identified relevant to a extreme weather event. Click on the map to visualize Tweets at different time points.\",\n",
    ")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(\n",
    "        geo_data, lat=\"lat\", lon=\"lon\", \n",
    "        hover_data=['full_text',\"created_at\",\"user_location\"],\n",
    "        color = 'retweet_count',\n",
    "        color_continuous_scale='teal',\n",
    "        zoom=1, \n",
    "        height=500\n",
    "    )\n",
    "        \n",
    "    layout = go.Layout(\n",
    "        margin=dict(l=10, r=10, t=20, b=10, pad=5),\n",
    "        plot_bgcolor=\"#171b26\",\n",
    "        paper_bgcolor=\"#171b26\",\n",
    "        clickmode=\"event+select\",\n",
    "        hovermode=\"closest\",\n",
    "        showlegend=False,\n",
    "        mapbox=go.layout.Mapbox(\n",
    "            accesstoken=mapbox_access_token,\n",
    "            bearing=10,\n",
    "            center=go.layout.mapbox.Center(\n",
    "                lat=filtered_data.lat.mean(), lon=filtered_data.lon.mean()\n",
    "            ),\n",
    "            pitch=5,\n",
    "            zoom=1,\n",
    "            style=\"mapbox://styles/plotlymapbox/cjvppq1jl1ips1co3j12b9hex\",\n",
    "        ),\n",
    "    )\n",
    "    return {\"data\": fig, \"layout\": layout}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
