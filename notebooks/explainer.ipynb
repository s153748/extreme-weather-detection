{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3fAMwHYAHCl"
   },
   "source": [
    "# Extreme weather detection\n",
    "\n",
    "**MASTER THESIS PROJECT**\n",
    "\n",
    "*Identification and Exploration of Extreme Weather Events From Twitter Data*\n",
    "\n",
    "**OBS: Should be run in Google Colab to take advantage of GPU**\n",
    "\n",
    "In this notebook, we load data from different sources. We build classifiers based on our labelled data set from https://crisislex.org/data-collections.html#CrisisLexT6 and with the use cases:\n",
    "\n",
    "1. Alberta floods https://en.wikipedia.org/wiki/2013_Alberta_floods \n",
    "2. Queensland floods https://en.wikipedia.org/wiki/Cyclone_Oswald\n",
    "\n",
    "The classifiers are build in three different categories within machine learning: Classic ML algorithms, Deep Learning and Transfer learning. After this follows the design of a localisation algorithm for localising the tweets in order to map them. \n",
    "\n",
    "The performance for each classifier is evaluated and then used for labelling tweets from the unlabelled data set collected by de Bruijn et al. and used to build the Global Flood Monitor (https://www.globalfloodmonitor.org/). We filtered out a subset of English tweets in the period 2016-2018.\n",
    "\n",
    "**Pipeline:**\n",
    "\n",
    "1. Data collection \n",
    "2. Data pre-processing\n",
    "\n",
    "Labelled data\n",
    "3. Build classifiers (labelled)\n",
    "4. Build localisation algorithm \n",
    "5. Build Visualisation\n",
    "\n",
    "Unlabelled data\n",
    "6. Classify tweets - use classifiers\n",
    "7. Localise tweets - use algorithm\n",
    "8. Visualise - explore and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T10:59:54.412969Z",
     "start_time": "2021-06-01T10:59:53.647672Z"
    },
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "nuRGU6zDAHCv",
    "outputId": "42aa9e06-f4f0-467d-cddc-29a4cf241de1"
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import string \n",
    "import itertools\n",
    "import requests\n",
    "import re\n",
    "import ast \n",
    "from ast import literal_eval\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import pathlib\n",
    "import textwrap\n",
    "import copy\n",
    "import calendar\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "#!pip install fastai==1.0.61\n",
    "from fastai.text import *\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import word_tokenize\n",
    "import collections\n",
    "import spacy\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "#!python -m spacy download en_core_web_lg\n",
    "#from gensim import models\n",
    "\n",
    "# scikit learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding, SimpleRNN\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.backend import clear_session\n",
    "from keras.models import model_from_json\n",
    "from keras_preprocessing.text import tokenizer_from_json\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from seaborn import color_palette\n",
    "import plotly.express as px \n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# mapping\n",
    "#!pip install geograpy3\n",
    "#import geograpy\n",
    "import folium\n",
    "from folium import FeatureGroup, LayerControl, plugins, Map, Marker\n",
    "from folium.plugins import FastMarkerCluster, MarkerCluster\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "# dash\n",
    "import dash\n",
    "import dash_table\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import dash_bootstrap_components as dbc\n",
    "from dash.dependencies import Input, Output, State\n",
    "import dash_dangerously_set_inner_html\n",
    "from jupyter_dash import JupyterDash\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uCZ28dvAHCx"
   },
   "source": [
    "# Data load and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEAn2Q0JAHC-"
   },
   "source": [
    "## Data clean functions\n",
    "\n",
    "First load data, then run\n",
    "\n",
    "- get user info\n",
    "- get tokens\n",
    "- add vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T11:00:20.145775Z",
     "start_time": "2021-06-01T11:00:20.134130Z"
    },
    "id": "tD5wMjUAAHC_"
   },
   "outputs": [],
   "source": [
    "# get user dataframe\n",
    "\n",
    "def clean_ascii(text):\n",
    "  # function to remove non-ASCII chars from data\n",
    "  return ''.join(i for i in text if ord(i) < 128)\n",
    "\n",
    "\n",
    "def get_userinfo(df_join1):  \n",
    "    \n",
    "    df_join1 = df_join1.rename(columns={'id': 'tweet_id'})   \n",
    "    df_join1['user_id'] = [df_join1['user'][i]['id'] for i in range(len(df_join1))]\n",
    "\n",
    "    users = [df_join1['user'][i] for i in range(len(df_join1))]\n",
    "    df_users = pd.DataFrame(users)\n",
    "    \n",
    "    cols =['id', 'id_str', 'name', 'screen_name', 'location', 'description', 'url',\n",
    "            'protected', 'followers_count', 'friends_count',\n",
    "           'listed_count', 'created_at', 'favourites_count', 'utc_offset',\n",
    "           'time_zone', 'geo_enabled', 'verified', 'statuses_count', 'lang']\n",
    "\n",
    "    df_users1 = df_users[cols].drop_duplicates().reset_index(drop=True)\n",
    "    df_users1 = df_users1.rename(columns={'id':'user_id','location':'user_location','screen_name':'user_name','name':'user_realname'})\n",
    "    \n",
    "    # merge user data with original data\n",
    "    df_join2 = pd.merge(df_users1[['user_id','user_name','user_realname','user_location']],df_join1, how='inner', on='user_id').reset_index(drop=True)\n",
    "    df_join2 = df_join2.replace(r'', np.NaN)\n",
    "    df_join2 = df_join2.drop_duplicates(subset='tweet_id')\n",
    "    \n",
    "    # get only english tweets\n",
    "    df_join2 = df_join2[df_join2['lang']=='en'].reset_index(drop=True)  \n",
    "    \n",
    "    # create variabel with removed non-ASCII and replaced RT \n",
    "    df_join2['text_clas'] = df_join2['full_text'].apply(clean_ascii).str.replace('RT','')\n",
    "\n",
    "    return df_join2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T11:00:37.602959Z",
     "start_time": "2021-06-01T11:00:37.595144Z"
    },
    "id": "i67ECV0lAHDB"
   },
   "outputs": [],
   "source": [
    "# get tokens of full text\n",
    "\n",
    "def get_tokens(df_join):\n",
    "    df_join['tokens'] = \"\"\n",
    "    \n",
    "    for i,content in enumerate(df_join['text_clas']):\n",
    "\n",
    "        if content:\n",
    "            text = content.translate(str.maketrans('', '', string.punctuation)).split() # split to tokens\n",
    "            sw = stopwords.words(\"english\") # set stopwords\n",
    "            wordnet_lemmatizer = WordNetLemmatizer() # set lemmatizer\n",
    "            words1= [t.lower() for t in text] # lower letters\n",
    "            words11 = [t for t in words1 if t not in sw]   # remove stopwords\n",
    "            words2 = [wordnet_lemmatizer.lemmatize(t) for t in words11]  # lemmatize\n",
    "            words3 = [x for x in words2 if not any(c.isdigit() for c in x)] # remove words with numbers\n",
    "\n",
    "            df_join['tokens'][i] = words3\n",
    "            \n",
    "    return df_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T11:00:58.594019Z",
     "start_time": "2021-06-01T11:00:58.574940Z"
    },
    "id": "jEsuB6Z7AHDC"
   },
   "outputs": [],
   "source": [
    "def add_vars(df):   \n",
    "   \n",
    "     # add hashtags variable\n",
    "    df['hashtags']= [re.findall(r\"(#\\w+)\", s) for s in df['full_text']]\n",
    "    \n",
    "    # add date variable\n",
    "    df['date'] = pd.to_datetime(df['created_at']).dt.date\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # add link\n",
    "    df['tweet_link'] = [f'https://twitter.com/{user_name}/status/{int(tweet_id)}' for user_name, tweet_id in zip(df['user_name'],df['tweet_id'])]\n",
    "    \n",
    "    # keep original tweet text \n",
    "    df['tweet'] = df['full_text']\n",
    "\n",
    "    # wrap full text\n",
    "    wrapper = textwrap.TextWrapper(width=50)\n",
    "    for i in range(len(df)):\n",
    "        df['full_text'][i] = \"<br>\".join(wrapper.wrap(text=df['full_text'][i]))\n",
    "        \n",
    "    ##### retweet variables #####\n",
    "    \n",
    "     # drop 'wrong' retweeted variable\n",
    "    df.drop(columns=['retweeted','retweet_count'],inplace=True)\n",
    "    \n",
    "    # get clean text by joining tokens to string\n",
    "    df['text_clean'] = [' '.join(s) for s in df['tokens']]\n",
    "    df = df.sort_values(by=['text_clean'])\n",
    "\n",
    "    # duplicates of full_text\n",
    "    dupl = df[df.duplicated(subset='text_clean',keep=False)].sort_values(\"text_clean\")\n",
    "    duplicates = dupl[['text_clean']].groupby(dupl[['text_clean']].columns.tolist()).size().reset_index().rename(columns={0:'duplicates'})\n",
    "  \n",
    "    # merge duplicates counts on df\n",
    "    df = pd.merge(df,duplicates,on='text_clean',how='outer').reset_index(drop=True)\n",
    "    df['duplicates'].fillna(0, inplace=True)\n",
    "\n",
    "    # get all duplicates - retweets\n",
    "    df_org = df[df['duplicates']>0].sort_values(by=['text_clean','created_at'])\n",
    "    df_org['retweeted'] = [True]*len(df_org)\n",
    "\n",
    "    # get first unique row - this is the original tweet\n",
    "    df_orgtweet = df_org.drop_duplicates(subset=['text_clean'],keep='first')\n",
    "    df_orgtweet['first_tweet'] = [True]*len(df_orgtweet)\n",
    "\n",
    "    # get tweet ids and mulitply by duplicates - to get original tweet id for retweeted tweets\n",
    "    ids = [[df_orgtweet['tweet_id'].iloc[i]]*int(df_orgtweet['duplicates'].iloc[i]) for i in range(len(df_orgtweet))] \n",
    "    List_flat = list(itertools.chain(*ids))\n",
    "    df_org['original_tweet_id'] = List_flat\n",
    "\n",
    "    df_fin = pd.merge(df_org[['tweet_id','original_tweet_id','retweeted']],df_orgtweet[['tweet_id','first_tweet']], on='tweet_id',how='left')\n",
    "\n",
    "    # all retweets are FALSE in first_tweet\n",
    "    df_fin['first_tweet']=df_fin['first_tweet'].replace(np.nan,False)\n",
    "\n",
    "    # merge\n",
    "    df = pd.merge(df,df_fin,on='tweet_id',how='left')\n",
    "\n",
    "    # all non retweeted are also TRUE for being first tweet\n",
    "    df['first_tweet']=df['first_tweet'].replace(np.nan,True)\n",
    "    df['retweeted']=df['retweeted'].replace(np.nan,False)\n",
    "\n",
    "    # all non retweeted tweets have their own id as original tweet id\n",
    "    df['original_tweet_id']= df['original_tweet_id'].fillna(df['tweet_id'])  \n",
    "    \n",
    "    # rename vars\n",
    "    df= df.rename(columns={'duplicates':'retweet_count','text_clean':'text_DL','text_clas':'text_TL'})\n",
    "\n",
    "    # add type variable\n",
    "    df['type'] = \"\"\n",
    "    for i in range(len(df)):\n",
    "        if df.retweeted[i]:\n",
    "            df['type'][i] = 'Retweet'      \n",
    "        else:\n",
    "            df['type'][i] = 'Tweet'   \n",
    "            df['tweet'][i] = df['tweet'][i].replace('RT ','').strip() \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3dIayMYAHDE"
   },
   "source": [
    "## Labelled tweets\n",
    "\n",
    "\n",
    "Adding variables\n",
    "- hashtags\n",
    "- retweet_count (= duplicates)\n",
    "- retweeted (if dupliates then it is a retweeted=True)\n",
    "- first tweet (if the first tweet in the group of duplicates OR if NOT retweeted, then TRUE)\n",
    "- original_tweet_id (for retweets this refers to first tweet, else it is just a copy of tweet id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T11:01:24.587357Z",
     "start_time": "2021-06-01T11:01:24.515907Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33951,
     "status": "ok",
     "timestamp": 1622038782319,
     "user": {
      "displayName": "Julie Petersen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq0qpPf2KC4BdN0egN2_TdTEglTbVVNL4VDKPWfw=s64",
      "userId": "00168236026826252484"
     },
     "user_tz": -120
    },
    "id": "EYr0E6Zsor_h",
    "outputId": "1c93a2d2-70d0-49ee-f470-b0f62a2ebd46"
   },
   "outputs": [],
   "source": [
    "# flooding use cases\n",
    "floods = ['2013_Alberta_floods','2013_Queensland_floods'] \n",
    "\n",
    "# load json with tweets for both floods\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for flood in floods:\n",
    "\n",
    "    df1 = pd.read_json(f'Labelled/{flood}_ids.json',lines=True)\n",
    "    df = pd.concat([df,df1]).reset_index(drop=True)\n",
    "\n",
    "print(f'Number of tweets: {len(df)}')\n",
    "\n",
    "# add userinfo, tokens, retweet variables etc.\n",
    "df2 = get_userinfo(df)\n",
    "df3 = get_tokens(df2)\n",
    "df4 = add_vars(df3)\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W2xHk_Z0tELb"
   },
   "outputs": [],
   "source": [
    "# add label\n",
    "# load csv with labels \n",
    "df_org = pd.DataFrame()\n",
    "\n",
    "for flood in floods:   \n",
    "\n",
    "    df1 = pd.read_csv(f'Labelled/{flood}.csv')\n",
    "    df1['tweet_id'] = [ int(t[1]) for t in df1['tweet id'].str.split(\"'\")]     \n",
    "    df_org = pd.concat([df_org,df1])\n",
    "\n",
    "df_org = df_org[df_org.columns[2:]]\n",
    "df_org = df_org.rename(columns={' label': 'relevant'})\n",
    "df_org = df_org.replace('off-topic',0).replace('on-topic',1)\n",
    "df_org = df_org[df_org['tweet_id'].isin(df4['tweet_id'])]\n",
    "\n",
    "# merge data sets\n",
    "df_join1 = pd.merge(df4,df_org,on='tweet_id',how='inner')\n",
    "df_join1['user_id'] = [df_join1['user'][i]['id'] for i in range(len(df_join1))]\n",
    "df_join1 = df_join1.drop_duplicates(subset=['tweet_id']).reset_index(drop=True)\n",
    "\n",
    "df = df_join1.copy()\n",
    "df.head(3)\n",
    "\n",
    "# export to csv\n",
    "#df_join1.to_json('Alberta_Queensland_floods.json',orient='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Examples\n",
    "for i in range(1):\n",
    "    print(i)\n",
    "    print(df[(df['relevant']==1) & (df['tweet'].str.contains('flood'))].iloc[i]['tweet'])\n",
    "    print(df[(df['relevant']==1) & (df['tweet'].str.contains('flood'))].iloc[i]['tweet_link'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare for classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-HTEXBYJzLB"
   },
   "outputs": [],
   "source": [
    "#get only first tweets - remove retweets\n",
    "\n",
    "df_clas = df[df['first_tweet']==True].reset_index(drop=True)\n",
    "df_clas = df_clas[['tweet_id','text_TL','text_DL','tokens','relevant']]\n",
    "\n",
    "# 1. Original tweets\n",
    "df_clas1 = df_clas.copy()\n",
    "\n",
    "# 2. replace chosen words\n",
    "df_clas2 = df_clas.copy()\n",
    "locs = ['yyc','ab','queensland','calgary','alberta','australia','canada','qld','nsw','edmonton','brisbane','bigwet']\n",
    "for loc in locs:\n",
    "  df_clas2['text_DL']= df_clas2['text_DL'].str.replace(loc,'')\n",
    "  df_clas2['text_TL']= df_clas2['text_TL'].str.replace(loc,'',flags=re.IGNORECASE)\n",
    "\n",
    "df_clas2['tokens'] = df_clas2['text_DL'].str.split()\n",
    "\n",
    "# 3. replace with place\n",
    "df_clas3 = df_clas.copy()\n",
    "\n",
    "V = 0.005*len(df_clas3)\n",
    "nlp =  spacy.load('en_core_web_lg')\n",
    "\n",
    "ids = [df_clas3['tokens'].iloc[i] for i in range(len(df_clas3))]\n",
    "   \n",
    "wordlist = list(itertools.chain(*ids))\n",
    "fd = FreqDist(wordlist)\n",
    "fd = {k: v for k, v in sorted(fd.items(), key=lambda item: item[1]) if v>V}\n",
    "fd_items = list(fd.items())[::-1]\n",
    "fd_keys = list(fd.keys())[::-1]\n",
    "\n",
    "places = nlp(' and '.join(fd_keys)).ents\n",
    "locs=[]\n",
    "for ent in places:\n",
    "    if ent.label_ == 'GPE':\n",
    "        locs.append(ent.text.lower())\n",
    "\n",
    "for loc in locs:\n",
    "  df_clas3['text_DL']= df_clas3['text_DL'].str.replace(loc,'')\n",
    "  df_clas3['text_TL']= df_clas3['text_TL'].str.replace(loc,'',flags=re.IGNORECASE)\n",
    "\n",
    "df_clas3['tokens'] = df_clas3['text_DL'].str.split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to calculate performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LUrFIYYkJvAB"
   },
   "outputs": [],
   "source": [
    "# performance\n",
    "\n",
    "def performance(y_test,y_pred):\n",
    "\n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    print('\\nConfusion matrix')\n",
    "    print(conf_mat)\n",
    "    TN =conf_mat[0][0]\n",
    "    FP =conf_mat[0][1]\n",
    "    FN =conf_mat[1][0]\n",
    "    TP =conf_mat[1][1]\n",
    "\n",
    "    acc = (TP+TN)/(TP+TN+FP+FN)\n",
    "    prec = TP/(TP+FP)\n",
    "    rec = TP/(TP+FN)\n",
    "    F1 = 2*rec*prec/(rec+prec)\n",
    "    \n",
    "    print('Accuracy:',(acc))\n",
    "    print('Precision: ', prec)\n",
    "    print('Recall: ', rec)\n",
    "    print('F1 Score: ', F1)\n",
    "\n",
    "    return conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:23:49.913022Z",
     "start_time": "2021-04-18T15:23:49.890834Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:31:46.495459Z",
     "start_time": "2021-04-18T15:31:46.017199Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.distplot(df['retweet_count'], color='#05264c', rug=True, kde=True)\n",
    "plt.xlabel('retweet_count',fontsize=14)\n",
    "plt.ylabel('density',fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:31:34.958932Z",
     "start_time": "2021-04-18T15:31:34.823499Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(y=df['retweet_count'],color='#05264c')\n",
    "plt.ylabel('retweet_count',fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:51:00.445603Z",
     "start_time": "2021-04-18T15:51:00.099799Z"
    }
   },
   "outputs": [],
   "source": [
    "def bar_chart(col,num):\n",
    "    df[col].value_counts()[:num].plot.barh(color='#05264c', figsize=(10, 5))\n",
    "    plt.title(f'Top {num} {col}s')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel('count')\n",
    "    plt.ylabel(col)\n",
    "    plt.tight_layout()\n",
    "\n",
    "bar_chart('user_location',20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:40:24.870622Z",
     "start_time": "2021-04-18T15:40:23.806600Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "pairplot = sns.pairplot(df[['retweet_count','user_followers_count']], diag_kind='kde', palette='#05264c')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T16:06:02.706634Z",
     "start_time": "2021-04-18T16:06:02.483681Z"
    }
   },
   "outputs": [],
   "source": [
    "# number of tweets over time\n",
    "month_count = np.unique(df['created_at_month'],return_counts=True)\n",
    "months = month_count[0]\n",
    "plt.figure(figsize=(10, 5))\n",
    "bins = np.arange(1,13)\n",
    "plt.hist(df['created_at_month'], bins=bins, color='#05264c')\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('month')\n",
    "plt.xticks(bins)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mhwnPEwAHDG"
   },
   "source": [
    "#  Classification \n",
    "\n",
    "\n",
    "First look at the distribution of tweets in the relevant/irrelevant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "executionInfo": {
     "elapsed": 344,
     "status": "ok",
     "timestamp": 1621606602632,
     "user": {
      "displayName": "Julie Petersen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq0qpPf2KC4BdN0egN2_TdTEglTbVVNL4VDKPWfw=s64",
      "userId": "00168236026826252484"
     },
     "user_tz": -120
    },
    "id": "5S854FMcAHDH",
    "outputId": "4997aa8b-83ba-4596-a665-1367d4de6333"
   },
   "outputs": [],
   "source": [
    "y = df_clas['relevant']\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "print('Number of tweets:', len(df_clas))\n",
    "print('Relevant:', sum(y), 'that is:' , round(sum(y)/len(y)*100,2), '%')\n",
    "print('Non-relevant:', len(y)-sum(y), 'that is:' ,round((len(y)-sum(y))/len(y)*100,2),'%')\n",
    "\n",
    "p1 = plt.barh(1,100,color='lightgreen')\n",
    "p2 = plt.barh(1,(len(y)-sum(y))/len(y)*100,color='firebrick')\n",
    "\n",
    "plt.legend((p1[0], p2[0]), ('Relevant', 'Non-relevant'),loc='upper center')\n",
    "plt.yticks([1,1.8])\n",
    "plt.title('Class balance',fontsize=16)\n",
    "#plt.axis('off')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6sHxOB4gAHDK"
   },
   "source": [
    "## Classic algorithms \n",
    "\n",
    "Here, we use the variable 'tokens'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DUkFwcmAHDK"
   },
   "outputs": [],
   "source": [
    "# define tfidf\n",
    "tfidf = TfidfVectorizer(preprocessor=' '.join)\n",
    "\n",
    "# test-train split\n",
    "size = [0.2,0.8]\n",
    "\n",
    "def text_fit(X, y, model,clf_model,size):   \n",
    "    X_c = model.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_c, y, random_state=0,test_size=size[0], train_size=size[1])\n",
    "    clf = clf_model.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    performance(y_test.tolist(), pred)\n",
    "   \n",
    "    return clf,X_c,model\n",
    "\n",
    "\n",
    "def print_words(model,clf_model,out):\n",
    "        w = model.get_feature_names()\n",
    "        if clf_model==clf_log:\n",
    "          coef = clf_model.coef_.tolist()[0]\n",
    "          STR = 'Coefficient'\n",
    "          coeff_df = pd.DataFrame({'Word' : w, STR : coef})\n",
    "          coeff_df = coeff_df.sort_values([STR, 'Word'], ascending=[0, 1])\n",
    "\n",
    "          print('')\n",
    "          print('-Top 5 relevant-')\n",
    "          print(coeff_df.head(5).to_string(index=False))\n",
    "          print('')\n",
    "          print('-Top 5 non-relevant-')        \n",
    "          print(coeff_df.tail(5).to_string(index=False))\n",
    "\n",
    "        else:\n",
    "          coef = clf_model.feature_importances_\n",
    "          STR = 'Score'\n",
    "          coeff_df = pd.DataFrame({'Word' : w, STR : coef})\n",
    "          coeff_df = coeff_df.sort_values([STR, 'Word'], ascending=[0, 1])\n",
    "          print('')\n",
    "          print('-Top 10 important-')\n",
    "          print(coeff_df.head(10).to_string(index=False))\n",
    "  \n",
    "        return coeff_df\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ez_FCIQNwpYJ"
   },
   "outputs": [],
   "source": [
    "# define for each set \n",
    "\n",
    "X1 = df_clas1['tokens']\n",
    "y1 = df_clas1['relevant']\n",
    "\n",
    "X2 = df_clas2['tokens']\n",
    "y2 = df_clas2['relevant']\n",
    "\n",
    "X3 = df_clas3['tokens']\n",
    "y3 = df_clas3['relevant']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqrBz7QGwaHP"
   },
   "source": [
    "*Logistic Regression*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1595,
     "status": "ok",
     "timestamp": 1621609471069,
     "user": {
      "displayName": "Julie Petersen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq0qpPf2KC4BdN0egN2_TdTEglTbVVNL4VDKPWfw=s64",
      "userId": "00168236026826252484"
     },
     "user_tz": -120
    },
    "id": "vRA-tBHVwdmS",
    "outputId": "33e9e12a-a2f3-40dd-b65d-39a1563e6107"
   },
   "outputs": [],
   "source": [
    "print('1. Original tweets')\n",
    "clf_log,X_c_log,tfidf_log = text_fit(X1, y1, tfidf, LogisticRegression(),size)\n",
    "_ = print_words(tfidf,clf_log,True)\n",
    "\n",
    "print('\\n2. Remove keywords')\n",
    "clf_log,X_c_log,tfidf_log = text_fit(X2, y2, tfidf, LogisticRegression(),size)\n",
    "_ = print_words(tfidf,clf_log,True)\n",
    "filename = 'LR.sav'\n",
    "pickle.dump(clf_log, open(filename, 'wb'))\n",
    "filename = 'LR_tfidf.sav'\n",
    "pickle.dump(tfidf_log, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "print('\\n3. Replace with place')\n",
    "clf_log,X_c_log,tfidf_log = text_fit(X3, y3, tfidf, LogisticRegression(),size)\n",
    "_ = print_words(tfidf,clf_log,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7HAaWREwe6h"
   },
   "source": [
    "*Random Forrest*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3555,
     "status": "ok",
     "timestamp": 1621609475945,
     "user": {
      "displayName": "Julie Petersen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq0qpPf2KC4BdN0egN2_TdTEglTbVVNL4VDKPWfw=s64",
      "userId": "00168236026826252484"
     },
     "user_tz": -120
    },
    "id": "qgc4JoLAwoVU",
    "outputId": "2565456f-5ad1-4e75-d416-e7d33cb914f0"
   },
   "outputs": [],
   "source": [
    "print('1. Original tweets')\n",
    "clf_RF,X_c_RF,tfidf_RF = text_fit(X1, y1, tfidf, RandomForestClassifier(max_depth=9, random_state=0),size)\n",
    "_ = print_words(tfidf,clf_RF,True)\n",
    "\n",
    "print('\\n2. Remove keywords')\n",
    "clf_RF,X_c_RF,tfidf_RF = text_fit(X2, y2, tfidf, RandomForestClassifier(max_depth=9, random_state=0),size)\n",
    "_ = print_words(tfidf,clf_RF,True)\n",
    "filename = 'RF.sav'\n",
    "pickle.dump(clf_RF, open(filename, 'wb'))\n",
    "filename = 'RF_tfidf.sav'\n",
    "pickle.dump(tfidf_RF, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "print('\\n3. Replace with place')\n",
    "clf_RF,X_c_RF,tfidf_RF = text_fit(X3, y3, tfidf, RandomForestClassifier(max_depth=9, random_state=0),size)\n",
    "_ = print_words(tfidf,clf_RF,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgrE49X3AHDM"
   },
   "source": [
    "## Deep learning\n",
    "\n",
    "We use the variable 'text_DL'\n",
    "\n",
    "\n",
    "### Pre-processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MqXHqs8RxvWn"
   },
   "outputs": [],
   "source": [
    "# load google news Word2Vec model \n",
    "word2vec_path = 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qlkGpVOAqM9e"
   },
   "outputs": [],
   "source": [
    "def split(df_clas):\n",
    "  # splitting data into test and train\n",
    "  data_train, data_test = train_test_split(df_clas, test_size=0.20, random_state=42)\n",
    "\n",
    "  print('Training vocabulary ')\n",
    "  # build training vocabulary\n",
    "  all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
    "  training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
    "  TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "  print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "  print(\"Max sentence length is %s\" % max(training_sentence_lengths))\n",
    "\n",
    "  print('\\nTesting vocabulary ')\n",
    "  # build testing vocabulary \n",
    "  all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n",
    "  test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\n",
    "  TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "  print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "  print(\"Max sentence length is %s\" % max(test_sentence_lengths))\n",
    "\n",
    "  return data_train,data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2FlKMQNooMF"
   },
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
    "    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, generate_missing=generate_missing))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6QS_AiCpUc3"
   },
   "outputs": [],
   "source": [
    "\n",
    "def final_embeddings(word2vec,data_train,data_test, MAX_SEQUENCE_LENGTH,EMBEDDING_DIM,RANDOM_STATE):\n",
    "\n",
    "  # get embeddings\n",
    "  training_embeddings = get_word2vec_embeddings(word2vec, data_train, generate_missing=True)\n",
    "\n",
    "  # Tokenize and transform to integer index\n",
    "  tokenizer = Tokenizer()\n",
    "  tokenizer.fit_on_texts(data_train['text_DL'])\n",
    "\n",
    "  X_train = tokenizer.texts_to_sequences(data_train['text_DL'])\n",
    "  X_test = tokenizer.texts_to_sequences(data_test['text_DL'])\n",
    "\n",
    "  vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "  train_word_index = tokenizer.word_index\n",
    "  print('Found %s unique tokens.' % len(train_word_index))\n",
    "\n",
    "  # Add pading to ensure all vectors have same dimensionality\n",
    "  X_train = pad_sequences(X_train, padding='post', maxlen=MAX_SEQUENCE_LENGTH)\n",
    "  y_train = data_train['relevant']\n",
    "\n",
    "  X_test = pad_sequences(X_test, padding='post', maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "  #train embeddings \n",
    "  train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "  y_test = data_test['relevant']\n",
    "\n",
    "  for word,index in train_word_index.items():\n",
    "      train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "\n",
    "  print(train_embedding_weights.shape)\n",
    "\n",
    "  return X_train, y_train, X_test, y_test, train_embedding_weights, vocab_size, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4t9lFNHBg-Em"
   },
   "outputs": [],
   "source": [
    "def predictions(data_test,model,tokenizer):\n",
    "\n",
    "    #tokenizer.fit_on_texts(df['full_text'].tolist())\n",
    "    sequences = tokenizer.texts_to_sequences(data_test['text_DL'])\n",
    "\n",
    "    MAX_SEQUENCE_LENGTH  = 30\n",
    "    new_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    predictions =model.predict_classes(new_data, batch_size=1024, verbose=1)\n",
    "\n",
    "    prediction_labels =  [i[0] for i in predictions.tolist()]\n",
    "            \n",
    "    return  prediction_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gSAZVk-Ncpcs"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vAtG8XJebXVR"
   },
   "outputs": [],
   "source": [
    "#parameters\n",
    "MAX_SEQUENCE_LENGTH = 30 # longest text in train set\n",
    "EMBEDDING_DIM = 300\n",
    "RANDOM_STATE = 42\n",
    "EPOCHS = 3\n",
    "BS = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbFh98vcz9DO"
   },
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "julbkw5SaiQB"
   },
   "outputs": [],
   "source": [
    "# Define CNN architecture\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "def train_CNN(df_clas,MAX_SEQUENCE_LENGTH,EMBEDDING_DIM,RANDOM_STATE,EPOCS,BS):\n",
    "\n",
    "  data_train, data_test = split(df_clas)\n",
    "  print('\\n')\n",
    "  X_train, y_train, X_test, y_test, train_embedding_weights,vocab_size,tokenizer = final_embeddings(word2vec,data_train,data_test,MAX_SEQUENCE_LENGTH,EMBEDDING_DIM,RANDOM_STATE)\n",
    "\n",
    "  model = Sequential()\n",
    "  model.add(layers.Embedding(vocab_size, EMBEDDING_DIM,weights=[train_embedding_weights], input_length=MAX_SEQUENCE_LENGTH))\n",
    "  model.add(layers.Conv1D(128, 5, activation='relu',kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "  model.add(layers.GlobalMaxPooling1D())\n",
    "  model.add(layers.Dense(32, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "  model.add(layers.Dense(10, activation='relu'))\n",
    "  model.add(layers.Dropout(0.1))\n",
    "  model.add(layers.Dense(1, activation='sigmoid'))\n",
    "  model.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "  print('\\n')\n",
    "  print(model.summary())\n",
    "\n",
    "  # Fit model\n",
    "\n",
    "  num_epochs = EPOCHS\n",
    "  batch_size = BS\n",
    "\n",
    "  hist = model.fit(X_train, \n",
    "                  y_train, \n",
    "                  epochs=num_epochs, \n",
    "                  validation_split=0.2, \n",
    "                  shuffle=True, \n",
    "                  batch_size=batch_size)\n",
    "\n",
    "\n",
    "  print('\\n')\n",
    "  loss, accuracy = model.evaluate(X_train, y_train, verbose=True)\n",
    "  print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "  loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "  print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "\n",
    "  predictions =model.predict_classes(X_test, batch_size=1024, verbose=1)\n",
    "  prediction_labels =  [i[0] for i in predictions.tolist()]\n",
    "\n",
    "  performance(y_test,prediction_labels)\n",
    "\n",
    "  return model, hist, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 19687,
     "status": "ok",
     "timestamp": 1621508694884,
     "user": {
      "displayName": "Julie Petersen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq0qpPf2KC4BdN0egN2_TdTEglTbVVNL4VDKPWfw=s64",
      "userId": "00168236026826252484"
     },
     "user_tz": -120
    },
    "id": "G92lNm1oWGE7",
    "outputId": "371c5af6-0794-4559-d9f8-55463257483e"
   },
   "outputs": [],
   "source": [
    "print('1. Original tweets\\n')\n",
    "CNN1, hist1,tokenizer1 = train_CNN(df_clas1,MAX_SEQUENCE_LENGTH,EMBEDDING_DIM,RANDOM_STATE,EPOCHS,BS)\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "%matplotlib inline\n",
    "plot_history(hist1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 36196,
     "status": "ok",
     "timestamp": 1621508713493,
     "user": {
      "displayName": "Julie Petersen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq0qpPf2KC4BdN0egN2_TdTEglTbVVNL4VDKPWfw=s64",
      "userId": "00168236026826252484"
     },
     "user_tz": -120
    },
    "id": "boY-C9vaX-iE",
    "outputId": "147d7801-7174-4b4d-e6c9-788a288acb5d"
   },
   "outputs": [],
   "source": [
    "print('2. Remove keywords\\n')\n",
    "\n",
    "CNN2,hist2,tokenizer2 = train_CNN(df_clas2,MAX_SEQUENCE_LENGTH,EMBEDDING_DIM,RANDOM_STATE,EPOCHS,BS)\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "%matplotlib inline\n",
    "plot_history(hist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 54484,
     "status": "ok",
     "timestamp": 1621508732625,
     "user": {
      "displayName": "Julie Petersen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq0qpPf2KC4BdN0egN2_TdTEglTbVVNL4VDKPWfw=s64",
      "userId": "00168236026826252484"
     },
     "user_tz": -120
    },
    "id": "Zwhlen52Zzg0",
    "outputId": "9f306c5b-0d50-435f-803a-2c2a046db280"
   },
   "outputs": [],
   "source": [
    "print('3. Replace with place\\n')\n",
    "CNN3, hist3,tokenizer3 = train_CNN(df_clas3,MAX_SEQUENCE_LENGTH,EMBEDDING_DIM,RANDOM_STATE,EPOCHS,BS)\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "%matplotlib inline\n",
    "plot_history(hist3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 868,
     "status": "ok",
     "timestamp": 1621508738851,
     "user": {
      "displayName": "Julie Petersen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq0qpPf2KC4BdN0egN2_TdTEglTbVVNL4VDKPWfw=s64",
      "userId": "00168236026826252484"
     },
     "user_tz": -120
    },
    "id": "Ej-5JjT_irOF",
    "outputId": "fb8448cc-ab03-4e79-8c94-9cf8883ba45e"
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "# MLP for Pima Indians Dataset Serialize to JSON and HDF5\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = CNN2.to_json()\n",
    "with open(\"CNN.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "tokenizer_json = tokenizer2.to_json()\n",
    "with open('CNN_tokenizer.json','w') as f:\n",
    "    f.write(tokenizer_json)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "CNN2.save_weights(\"CNN.h5\")\n",
    "print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0p8-812mAw9"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def tsne_plot(model):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xl3QuzpwMjNM"
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(corpus, size=100, window=20, min_count=500, workers=4)\n",
    "tsne_plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKh6bu5gAHDS"
   },
   "source": [
    "##  Transfer learning \n",
    "\n",
    "Using variable 'text_TL'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bo99guT1AHDT"
   },
   "source": [
    "### ULMFiT \n",
    "\n",
    "Build with inspiration from the following:\n",
    "\n",
    "https://towardsdatascience.com/transfer-learning-in-nlp-for-tweet-stance-classification-8ab014da8dde\n",
    "\n",
    "https://github.com/floleuerer/fastai_ulmfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p7umuoklAHDT"
   },
   "outputs": [],
   "source": [
    "def get_data(df_clas):\n",
    "  size = [0.2,0.8]\n",
    "\n",
    "  X = df_clas['text_TL']\n",
    "  y = df_clas['relevant']\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0,test_size=size[0], train_size=size[1])\n",
    "\n",
    "  df_train = pd.DataFrame()\n",
    "  df_test = pd.DataFrame()\n",
    "\n",
    "  df_train['relevant'] = y_train\n",
    "  df_train['text'] = X_train \n",
    "\n",
    "  df_test['relevant'] = y_test\n",
    "  df_test['text'] = X_test\n",
    "\n",
    "  return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hd5lFVRNQlbx"
   },
   "outputs": [],
   "source": [
    "# define for three different text variables\n",
    "\n",
    "df_train1, df_test1 = get_data(df_clas1)\n",
    "df_train2, df_test2 = get_data(df_clas2)\n",
    "df_train3, df_test3 = get_data(df_clas3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0P0TFmyNAHDU"
   },
   "source": [
    "#### Language model fine-tuning \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ifeJl0NAHDU"
   },
   "outputs": [],
   "source": [
    "# Language model data\n",
    "\n",
    "def langmodel_data(df_train,df_test):\n",
    "  data_lm = TextLMDataBunch.from_df('', train_df=df_train, valid_df=df_test,min_freq=1,text_cols='text')\n",
    "  data_lm.show_batch()\n",
    "\n",
    "  # Save the language model data for re-use\n",
    "  data_lm.save()\n",
    "  \n",
    "  return data_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ub0_MSwSAHDU"
   },
   "outputs": [],
   "source": [
    "def langmodel_train(data_lm,enc_name):\n",
    "\n",
    "  # Language model\n",
    "    learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5)\n",
    "\n",
    "    learn.lr_find(start_lr=1e-8, end_lr=1e2)\n",
    "    learn.recorder.plot()\n",
    "\n",
    "    #learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5)\n",
    "    learn.fit_one_cycle(cyc_len=1, max_lr=1e-2,moms=(0.8, 0.7))\n",
    "\n",
    "    learn.unfreeze()\n",
    "    learn.fit_one_cycle(cyc_len=8, max_lr=1e-2,moms=(0.8, 0.7))\n",
    "\n",
    "    # Save the fine-tuned encoder\n",
    "    learn.save_encoder(enc_name)\n",
    "\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hw4xaqUiY6GQ"
   },
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EADiWSE5YxAk"
   },
   "outputs": [],
   "source": [
    "# Classifier model data\n",
    "def classifier_data(df_train,df_test,data_lm):\n",
    "  data_clas = TextClasDataBunch.from_df('', train_df=df_train, valid_df=df_test, vocab=data_lm.train_ds.vocab, min_freq=1,bs=32)\n",
    "  data_clas.save()\n",
    "\n",
    "  return data_clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AcG46fFiYxD8"
   },
   "outputs": [],
   "source": [
    "# Classifier\n",
    "\n",
    "def classifier_train(data_clas,enc_name):\n",
    "  classifier = text_classifier_learner(data_clas, AWD_LSTM,drop_mult=0.5)\n",
    "  classifier.load_encoder(enc_name)\n",
    "\n",
    "  classifier.lr_find(start_lr=1e-8, end_lr=1e2)\n",
    "  classifier.recorder.plot()\n",
    "\n",
    "  classifier.fit_one_cycle(cyc_len=1, max_lr=1e-2, moms=(0.8, 0.7))\n",
    "  classifier.recorder.plot_losses()\n",
    "  classifier.freeze_to(-2)\n",
    "  classifier.fit_one_cycle(1, slice(1e-4,1e-2), moms=(0.8,0.7))\n",
    "  classifier.freeze_to(-3)\n",
    "  classifier.fit_one_cycle(1, slice(1e-5,5e-3), moms=(0.8,0.7))\n",
    "  classifier.unfreeze()\n",
    "  classifier.fit_one_cycle(3, slice(1e-5,1e-2), moms=(0.8,0.7))\n",
    "\n",
    "  return classifier\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHapwzmSAHDW"
   },
   "source": [
    "#### Build models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rM2ChIN6Z7Ur"
   },
   "source": [
    "*Original tweets*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 996
    },
    "executionInfo": {
     "elapsed": 683910,
     "status": "ok",
     "timestamp": 1621509446818,
     "user": {
      "displayName": "Julie Petersen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq0qpPf2KC4BdN0egN2_TdTEglTbVVNL4VDKPWfw=s64",
      "userId": "00168236026826252484"
     },
     "user_tz": -120
    },
    "id": "5VuRqh7gAHDX",
    "outputId": "2994a148-298d-4bd2-f9af-8db270fe87b4"
   },
   "outputs": [],
   "source": [
    "# language learner \n",
    "data_lm1 = langmodel_data(df_train1,df_test1)\n",
    "learn1 = langmodel_train(data_lm1,'ft_enc1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 937
    },
    "executionInfo": {
     "elapsed": 1322284,
     "status": "ok",
     "timestamp": 1621510085200,
     "user": {
      "displayName": "Julie Petersen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq0qpPf2KC4BdN0egN2_TdTEglTbVVNL4VDKPWfw=s64",
      "userId": "00168236026826252484"
     },
     "user_tz": -120
    },
    "id": "SBopa8mHdlos",
    "outputId": "a77b17ee-12a4-4491-8f28-de737ec33219"
   },
   "outputs": [],
   "source": [
    "# classification\n",
    "data_clas1 = classifier_data(df_train1,df_test1,data_lm1)\n",
    "classifier1 = classifier_train(data_clas1,'ft_enc1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWMwOfngc5jQ"
   },
   "outputs": [],
   "source": [
    "# export model\n",
    "classifier1.export('ULMFiT1.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVx8BGBNaGLC"
   },
   "source": [
    "*Remove keywords*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 963
    },
    "executionInfo": {
     "elapsed": 1984721,
     "status": "ok",
     "timestamp": 1621510747644,
     "user": {
      "displayName": "Julie Petersen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq0qpPf2KC4BdN0egN2_TdTEglTbVVNL4VDKPWfw=s64",
      "userId": "00168236026826252484"
     },
     "user_tz": -120
    },
    "id": "mKCLa1VOZjLi",
    "outputId": "f4e1104a-9488-43a2-cf8e-0a1194da2b31"
   },
   "outputs": [],
   "source": [
    "# language learner \n",
    "data_lm2 = langmodel_data(df_train2,df_test2)\n",
    "learn2 = langmodel_train(data_lm2,'ft_enc2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 937
    },
    "executionInfo": {
     "elapsed": 2617465,
     "status": "ok",
     "timestamp": 1621511380393,
     "user": {
      "displayName": "Julie Petersen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq0qpPf2KC4BdN0egN2_TdTEglTbVVNL4VDKPWfw=s64",
      "userId": "00168236026826252484"
     },
     "user_tz": -120
    },
    "id": "7pdiyFMQZj4K",
    "outputId": "4b8ec881-bc6f-4e92-8cf2-2ea337a872f8"
   },
   "outputs": [],
   "source": [
    "# classification\n",
    "data_clas2 = classifier_data(df_train2,df_test2,data_lm2)\n",
    "classifier2 = classifier_train(data_clas2,'ft_enc2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-F0Sqkcnj1Eg"
   },
   "outputs": [],
   "source": [
    "classifier2.export('ULMFiT2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b46SZY1XaJyU"
   },
   "source": [
    "*Replace places*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 967
    },
    "executionInfo": {
     "elapsed": 3282217,
     "status": "ok",
     "timestamp": 1621512045151,
     "user": {
      "displayName": "Julie Petersen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq0qpPf2KC4BdN0egN2_TdTEglTbVVNL4VDKPWfw=s64",
      "userId": "00168236026826252484"
     },
     "user_tz": -120
    },
    "id": "YjxVHuA9Zx44",
    "outputId": "72b3b704-f991-4b39-a533-27be82428483"
   },
   "outputs": [],
   "source": [
    "# language learner \n",
    "data_lm3 = langmodel_data(df_train3,df_test3)\n",
    "learn3 = langmodel_train(data_lm3,'ft_enc3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 937
    },
    "executionInfo": {
     "elapsed": 3915368,
     "status": "ok",
     "timestamp": 1621512678307,
     "user": {
      "displayName": "Julie Petersen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq0qpPf2KC4BdN0egN2_TdTEglTbVVNL4VDKPWfw=s64",
      "userId": "00168236026826252484"
     },
     "user_tz": -120
    },
    "id": "D1rweIFkZx-b",
    "outputId": "0a1f3ed2-b289-4b74-d5b1-9fe8b9c8865f"
   },
   "outputs": [],
   "source": [
    "# classification\n",
    "data_clas3 = classifier_data(df_train3,df_test3,data_lm3)\n",
    "classifier3 = classifier_train(data_clas3,'ft_enc3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mU3o2a1EZyEi"
   },
   "outputs": [],
   "source": [
    "classifier3.export('ULMFiT3.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "executionInfo": {
     "elapsed": 3985546,
     "status": "ok",
     "timestamp": 1621512748491,
     "user": {
      "displayName": "Julie Petersen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq0qpPf2KC4BdN0egN2_TdTEglTbVVNL4VDKPWfw=s64",
      "userId": "00168236026826252484"
     },
     "user_tz": -120
    },
    "id": "MYLUvVwX4Mxi",
    "outputId": "aaa81644-89ff-4938-9c6a-8c8770f70590"
   },
   "outputs": [],
   "source": [
    "# get predictions\n",
    "#cl = load_learner('','ULMFiT1.pkl')\n",
    "classifier1.data.add_test(df_test1['text'])\n",
    "preds, _ = classifier1.get_preds(ds_type=DatasetType.Test)\n",
    "targets =df_test1['relevant']\n",
    "predictions = np.argmax(preds, axis=1)\n",
    "\n",
    "_ = performance(targets,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "executionInfo": {
     "elapsed": 4054874,
     "status": "ok",
     "timestamp": 1621512817823,
     "user": {
      "displayName": "Julie Petersen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq0qpPf2KC4BdN0egN2_TdTEglTbVVNL4VDKPWfw=s64",
      "userId": "00168236026826252484"
     },
     "user_tz": -120
    },
    "id": "rRZvWPpHm_Ha",
    "outputId": "c234f32f-1fe7-44e0-e356-2a0a935b97c5"
   },
   "outputs": [],
   "source": [
    "# get predictions\n",
    "#cl = load_learner('','ULMFiT1.pkl')\n",
    "classifier2.data.add_test(df_test2['text'])\n",
    "preds, _ = classifier2.get_preds(ds_type=DatasetType.Test)\n",
    "targets =df_test2['relevant']\n",
    "predictions = np.argmax(preds, axis=1)\n",
    "\n",
    "_ = performance(targets,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "executionInfo": {
     "elapsed": 4124696,
     "status": "ok",
     "timestamp": 1621512887650,
     "user": {
      "displayName": "Julie Petersen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq0qpPf2KC4BdN0egN2_TdTEglTbVVNL4VDKPWfw=s64",
      "userId": "00168236026826252484"
     },
     "user_tz": -120
    },
    "id": "Zo-BJYDHm_lW",
    "outputId": "9d9cc998-3f0e-4a6a-f5c0-a23927e70441"
   },
   "outputs": [],
   "source": [
    "# get predictions\n",
    "#cl = load_learner('','ULMFiT1.pkl')\n",
    "classifier3.data.add_test(df_test3['text'])\n",
    "preds, _ = classifier3.get_preds(ds_type=DatasetType.Test)\n",
    "targets =df_test3['relevant']\n",
    "predictions = np.argmax(preds, axis=1)\n",
    "\n",
    "_ = performance(targets,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QL_P4Y3JAHDb"
   },
   "source": [
    "# Localisation \n",
    "\n",
    "\n",
    "\n",
    "In this section, we look into getting a location for the tweets. This is prioritesed in the four levels stated here.\n",
    "\n",
    "*Note that the coordinates attributes is formatted as [LONGITUDE, latitude], while the geo attribute is formatted as [latitude, LONGITUDE].\n",
    "\n",
    "1. Geotagged coordinates\n",
    "2. Geotagged place\n",
    "3. Geoparsed from text\n",
    "4. Registered user location\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSWE9nB4AHDd"
   },
   "source": [
    "## Geo (coordinate) attribute  and Place attribute \n",
    "\n",
    "- get centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_14HIv1OAHDe"
   },
   "outputs": [],
   "source": [
    "def get_centers(df_rel):\n",
    "    \n",
    "    #users with places and not geo\n",
    "    place_df = df_rel[(df_rel['geo'].isna()) & (df_rel['place'].notna())].reset_index(drop=True)\n",
    "\n",
    "    # add place_id\n",
    "    place_df['place_id'] =[place_df['place'][i]['id'] for i in range(len(place_df))]\n",
    "\n",
    "    # get dataframe with places metadata\n",
    "    places = [place_df['place'][i] for i in range(len(place_df))]\n",
    "    df_places= pd.DataFrame(places)\n",
    "    df_places= df_places.rename(columns={'id':'place_id'})\n",
    "    df_places = df_places.drop_duplicates(subset=['place_id'])\n",
    "\n",
    "    # merge to get all place details\n",
    "    place_df2 =pd.merge(place_df.drop(columns=['place']),df_places, on='place_id',how='inner')\n",
    "\n",
    "    cols=['tweet_id','place_type', 'name',\n",
    "           'full_name', 'country',\n",
    "           'bounding_box']\n",
    "    # merge to all data\n",
    "\n",
    "    df_rel = pd.merge(df_rel,place_df2[cols],on='tweet_id',how='left').reset_index(drop=True)\n",
    "    df_rel['geo'] = df_rel['geo'].fillna(df_rel['bounding_box'])\n",
    "    \n",
    "    # get centers of polygons\n",
    "    # create variable with type point or polygon\n",
    "    # create 'center' variable with either point or center of polygon\n",
    "\n",
    "    df_try =  df_rel[~df_rel['geo'].isna()].reset_index(drop=True)\n",
    "    df_try['location_type'] = [g['type'] for g in df_try['geo']]\n",
    "    df_try['coordinates2'] = [g['coordinates'] for g in df_try['geo']]\n",
    "\n",
    "    # get centers of polygon or just point\n",
    "    centers = []\n",
    "\n",
    "    for i in range(len(df_try)):\n",
    "        if df_try['location_type'][i]=='Polygon':      \n",
    "            center = list(np.average(df_try['coordinates2'][i][0],axis=0))[::-1]\n",
    "            centers.append(center)\n",
    "        else:\n",
    "            centers.append(df_try['coordinates2'][i])\n",
    "    df_try['centers'] = centers\n",
    "\n",
    "    # merge with df_rel\n",
    "    df_rel = pd.merge(df_rel,df_try[['tweet_id','location_type','centers']],on='tweet_id',how='left').reset_index(drop=True)\n",
    "    \n",
    "    return df_rel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcIJDtDFAHDg"
   },
   "source": [
    "## Geo-parsing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GowgsYH-AHDg"
   },
   "source": [
    "1) toponym recognition\n",
    "\n",
    "Use spacy entitity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOiY-qULAHDh"
   },
   "outputs": [],
   "source": [
    "def geoparsing(df_rel):\n",
    "\n",
    "    #import en_core_web_sm\n",
    "    #nlp = en_core_web_sm.load()\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "    df_rel['geoparsing'] = ''\n",
    "\n",
    "    for i in range(len(df_rel)):      \n",
    "        \n",
    "        if (i % 2500) == 0:\n",
    "            print(i)\n",
    "            \n",
    "        places = nlp(df_rel['tweet'][i]).ents\n",
    "        locs=[]\n",
    "        for ent in places:\n",
    "            if ent.label_ == 'GPE':\n",
    "                locs.append(ent.text.lower())\n",
    "            if len(locs)!=0:\n",
    "                df_rel['geoparsing'][i] = locs\n",
    "            else:\n",
    "                df_rel['geoparsing'][i] = ['NAN']\n",
    "\n",
    "    # get count of matches\n",
    "    df_rel['geoparsing_count'] = [len(m) for m in df_rel['geoparsing']]\n",
    "\n",
    "    #replace empty\n",
    "    df_rel['geoparsing'] = [np.unique(row) for row in df_rel['geoparsing']]\n",
    "\n",
    "    return df_rel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oL65eWh8AHDi"
   },
   "source": [
    "2) Look up table for coordinates\n",
    "\n",
    "- user location \n",
    "- geoparsed locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2VoUqkcAHDi"
   },
   "outputs": [],
   "source": [
    "# get unique user locations appearing over 10 times in list\n",
    "\n",
    "def user_locs(df_rel,thres):\n",
    "\n",
    "    df_rel['user_location_lower'] = ''\n",
    "\n",
    "    for i,u in enumerate(df_rel['user_location'].str.lower()):\n",
    "        try:\n",
    "            df_rel['user_location_lower'][i] = u.split(',')[0] \n",
    "        except:\n",
    "            None\n",
    "\n",
    "    # get unique user locations mentioned more than threshold\n",
    "\n",
    "    values, counts = np.unique(list(df_rel['user_location_lower']),return_counts=True)\n",
    "\n",
    "    vals = values[counts>=thres]\n",
    "    user_list1 = vals[(vals!='') & (vals!='NAN')]\n",
    "\n",
    "    user_list = list(np.unique([u.split(',')[0] for u in user_list1]))\n",
    "    \n",
    "    return user_list\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_SFfYbOSAHDj"
   },
   "outputs": [],
   "source": [
    "def geo_locs(df_rel,thres):\n",
    "       \n",
    "    # get list of geoparsed locations\n",
    "    gps = list(df_rel['geoparsing'])\n",
    "    flat_gps = list(itertools.chain(*gps))\n",
    "\n",
    "\n",
    "    # get unique geoparsed location mentioned more than threshold\n",
    "    values, counts = np.unique(flat_gps,return_counts=True)\n",
    "\n",
    "    vals = values[counts>=thres]\n",
    "    gp_list = list(vals[(vals!='') & (vals!='NAN')])\n",
    "    \n",
    "    return gp_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQaUAjdXAHDj"
   },
   "outputs": [],
   "source": [
    "def lookup_table(user_list,gp_list):\n",
    "     \n",
    "    # initialize geolocator\n",
    "    geolocator = Nominatim(user_agent='my_app') \n",
    "    geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n",
    "\n",
    "    # create look up table for unique locations - combining user and geoparsed\n",
    "    df_lookup = pd.DataFrame()\n",
    "    df_lookup['location'] = list(np.unique(user_list+gp_list))\n",
    "\n",
    "    # get coordinates\n",
    "    df_lookup['coordinates']= ''\n",
    "\n",
    "    for i,loc in enumerate(df_lookup['location']):  \n",
    "\n",
    "        if (i % 100) == 0:\n",
    "            print(i)\n",
    "\n",
    "        if str(loc)!='nan':\n",
    "            try:\n",
    "                df_lookup['coordinates'][i]=[geolocator.geocode(loc).latitude,geolocator.geocode(loc).longitude]\n",
    "            except:\n",
    "                df_lookup['coordinates'][i] = np.nan\n",
    "        else:\n",
    "            df_lookup['coordinates'][i] = np.nan\n",
    "\n",
    "    df_lookup=df_lookup.dropna()\n",
    "    \n",
    "    return df_lookup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b81IokikAHDk"
   },
   "outputs": [],
   "source": [
    "def add_coords(df_rel,df_lookup):\n",
    "    # add index to flatten out geoparsing - when more than one location\n",
    "    df_rel['index'] = df_rel.index\n",
    "\n",
    "    b_flat = pd.DataFrame([[i, x] \n",
    "                   for i, y in df_rel['geoparsing'].apply(list).iteritems() \n",
    "                       for x in y], columns=list('IB'))\n",
    "    b_flat = b_flat.set_index('I')\n",
    "\n",
    "    df_match = df_rel[['index']].merge(b_flat, left_index=True, right_index=True)\n",
    "    df_match.rename(columns={'B':'location'},inplace=True)\n",
    "\n",
    "    # look up geoparse locations and join to main data set df_rel\n",
    "    df_merge1 = pd.merge(df_match,df_lookup,on='location').sort_values(by='index')\n",
    "    hej = pd.DataFrame(df_merge1.groupby(['index'])['coordinates'].apply(list)).reset_index()\n",
    "    hej.rename(columns={'coordinates':'gp_coords'},inplace=True)\n",
    "    df_rel2 = pd.merge(df_rel,hej,on='index',how='left')\n",
    "    \n",
    "    # get user coordinates using look up table\n",
    "    df_rel2['location'] = df_rel2['user_location_lower']\n",
    "    df_rel3 = pd.merge(df_rel2,df_lookup.rename(columns={'coordinates':'user_coordinates'}),on='location',how='left')\n",
    "    df_rel3.drop(columns=['location','index'],inplace=True)\n",
    "\n",
    "\n",
    "    # drop if does not have any coordinates available\n",
    "    df_rel4 = df_rel3.dropna(subset=['centers','gp_coords', 'user_coordinates'],how='all').reset_index(drop=True)\n",
    "    \n",
    "    return df_rel4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2) toponym resolution*\n",
    "\n",
    "\n",
    "Geoparse coords\n",
    "    - if geoparse coords have two locations closer than 1500 km, randomnly choose one\n",
    "    - if geoparse coords have more than two then take two with shortest distance, if below 1500 km, randomnly choose one\n",
    "    - else just use coords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JacKNBc6AHDl"
   },
   "outputs": [],
   "source": [
    "def final_geocoords(df_rel4):\n",
    "    \n",
    "    # define dataframe with only geoparsing locations\n",
    "    df_gp =df_rel4[df_rel4['gp_coords'].notna()].reset_index(drop=True)\n",
    "\n",
    "    # get count of matches\n",
    "    df_gp['gp_count'] = [len(m) for m in df_gp['gp_coords']]\n",
    "  \n",
    "    earth = 6371000/1000\n",
    "\n",
    "    # define new variable to have the chosen coordinates\n",
    "    df_gp['gp_coords1'] = ''\n",
    "    df_gp['distance'] = ''\n",
    "\n",
    "    for j in range(len(df_gp)):\n",
    "\n",
    "        x = df_gp['gp_coords'].iloc[j]\n",
    "        count = df_gp['gp_count'].iloc[j]\n",
    "\n",
    "        # if more than 2 locations, get two closest\n",
    "        if count>2:    \n",
    "            xx = [[radians(i[0]),radians(i[1])] for i in x]\n",
    "            mat = np.round(haversine_distances(xx, xx)*earth)\n",
    "            idx = list(np.unravel_index(np.where(mat!=0, mat, mat.max()+1).argmin(), mat.shape))\n",
    "\n",
    "            if mat[idx[0],idx[1]]<1500:\n",
    "              rand = np.random.choice([0,1])\n",
    "              new_coords = x[idx[rand]]\n",
    "            else:\n",
    "              new_coords = 'NAN'\n",
    "\n",
    "            df_gp['gp_coords1'].iloc[j]=new_coords\n",
    "            df_gp['distance'].iloc[j] = mat[idx[0],idx[1]]\n",
    "\n",
    "\n",
    "            #new_coords = [np.average([x[idx[0]][0],x[idx[1]][0]]),np.average([x[idx[0]][1],x[idx[1]][1]])] \n",
    "            \n",
    "        elif count==2:\n",
    "     \n",
    "            xx = [[radians(i[0]),radians(i[1])] for i in x]\n",
    "            mat = np.round(haversine_distances(xx, xx)*earth)\n",
    "\n",
    "            if mat[0,1]<1500:\n",
    "              rand = np.random.choice([0,1])\n",
    "              new_coords = x[rand]\n",
    "            else:\n",
    "              new_coords = 'NAN'\n",
    "\n",
    "          #  new_coords = [np.average([x[0][0],x[1][0]]),np.average([x[0][1],x[1][1]])]\n",
    "            df_gp['gp_coords1'].iloc[j]=new_coords\n",
    "            df_gp['distance'].iloc[j] = mat[0,1]\n",
    "            \n",
    "    \n",
    "        elif count==1:   \n",
    "            df_gp['gp_coords1'].iloc[j] = x[0]\n",
    "            df_gp['distance'].iloc[j] = 0\n",
    "          \n",
    "       \n",
    "        \n",
    "    # merge with final dataframe\n",
    "    df_rel5 = pd.merge(df_rel4,df_gp[['tweet_id','gp_coords1','distance']],on='tweet_id',how='left').reset_index(drop=True)\n",
    "            \n",
    "    return df_rel5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. 'centers' if location_type = Point\n",
    "2. 'centers' if location_type = Polygon\n",
    "3. geoparse coords\n",
    "5. user coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWw4uC0pAHDm"
   },
   "outputs": [],
   "source": [
    "def final_data(df_rel5):\n",
    "\n",
    "    #replace np.nan with NAN - only way to make if statement\n",
    "    df_rel5['gp_coords1'] = df_rel5['gp_coords1'].replace(np.nan,'NAN')\n",
    "    df_rel5['user_coordinates'] = df_rel5['user_coordinates'].replace(np.nan,'NAN')\n",
    "\n",
    "\n",
    "    # define columns for final coordinates - type\n",
    "    # 1 'centers' if location_type = Point\n",
    "    # 2  'centers' if location_type = Polygon\n",
    "    # 3  geoparse coords\n",
    "    # 4 user coords\n",
    "    # NAN\n",
    "\n",
    "    df_rel5['final_coords'] = ''\n",
    "    df_rel5['localization'] = ''\n",
    "\n",
    "    for i in range(len(df_rel5)):\n",
    "\n",
    "            # 'centers' if location_type = Point\n",
    "        if df_rel5['location_type'].iloc[i] == 'Point':\n",
    "            df_rel5['final_coords'].iloc[i] = df_rel5['centers'].iloc[i]\n",
    "            df_rel5['localization'].iloc[i]  = 'Geotagged coordinates'\n",
    "\n",
    "            # 'centers' if location_type = Polygon\n",
    "\n",
    "        elif df_rel5['location_type'].iloc[i] == 'Polygon':\n",
    "            df_rel5['final_coords'].iloc[i] = df_rel5['centers'].iloc[i]\n",
    "            df_rel5['localization'].iloc[i]  = 'Geotagged place'\n",
    "\n",
    "            # geoparse coords \n",
    "        elif df_rel5['gp_coords1'].iloc[i] !='NAN':\n",
    "            df_rel5['final_coords'].iloc[i] = df_rel5['gp_coords1'].iloc[i]\n",
    "            df_rel5['localization'].iloc[i]  = 'Geoparsed from Tweet'\n",
    "\n",
    "            # user coords\n",
    "        elif df_rel5['user_coordinates'].iloc[i] !='NAN':\n",
    "            df_rel5['final_coords'].iloc[i] = df_rel5['user_coordinates'].iloc[i]\n",
    "            df_rel5['localization'].iloc[i]  = 'Registered user location'\n",
    "\n",
    "             # nan\n",
    "        else:\n",
    "            df_rel5['final_coords'].iloc[i] = 'NAN'\n",
    "            df_rel5['localization'].iloc[i]  = 'NAN'\n",
    "\n",
    "\n",
    "    df_rel5 = df_rel5[df_rel5['final_coords']!='NAN'].reset_index(drop=True)\n",
    "    \n",
    "    return df_rel5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knFrRwg9AHDn"
   },
   "source": [
    "Some coords are duplicates. In order to map them, we add noise to them using kernel density estimation. \n",
    "\n",
    "Mean = coordinates; Sigma = [0.7,0][0,0.7], N = number of duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qXWibFijAHDn"
   },
   "outputs": [],
   "source": [
    "\n",
    "def addnoise_coords(final_df):\n",
    "\n",
    "    # get as string\n",
    "    final_df['final_coords_str'] = final_df['final_coords'].astype(str)\n",
    "\n",
    "    # duplicates of full_text\n",
    "    dupl = final_df[final_df.duplicated(subset='final_coords_str',keep=False)].sort_values(\"final_coords_str\")\n",
    "    print(f'Number of duplicate coordinates: {len(dupl)} corresponding to {np.round(len(dupl)/len(final_df)*100,1)} %')\n",
    "    duplicates = dupl[['final_coords_str']].groupby(dupl[['final_coords_str']].columns.tolist()).size().reset_index().rename(columns={0:'duplicates'})\n",
    "    \n",
    "    \n",
    "     # merge duplicates counts on df\n",
    "    final_df = pd.merge(final_df,duplicates,on='final_coords_str',how='outer').reset_index(drop=True)\n",
    "   \n",
    "    final_df['duplicates'].fillna(0, inplace=True)\n",
    "    \n",
    "    \n",
    "    duplicates['final_coords'] =  [literal_eval(s) for s in duplicates['final_coords_str']]\n",
    "    \n",
    "    # randomize coords\n",
    "    duplicates['new_coords'] = ''\n",
    "\n",
    "    for i in range(len(duplicates)):\n",
    "        start = duplicates['final_coords'][i]    \n",
    "        N=duplicates['duplicates'][i]\n",
    "\n",
    "        cov = [[0.7, 0], [0, 0.7]]\n",
    "        merged = np.random.multivariate_normal(start, cov, N)\n",
    "        duplicates['new_coords'][i] = merged \n",
    "        \n",
    "    # get list of randomized coords \n",
    "    ids2 = list(duplicates['new_coords'])\n",
    "    List_flat2 = list(itertools.chain(*ids2))\n",
    "    List_flat2 = [list(l) for l in List_flat2]\n",
    "    \n",
    "    # get all duplicates and add variable with new random coords\n",
    "    final_df2 = final_df[final_df['duplicates']>0].sort_values(by=['final_coords_str']).reset_index(drop=True)\n",
    "    final_df2['random_coords'] = List_flat2\n",
    "\n",
    "    # merge\n",
    "    final_df = pd.merge(final_df.drop(columns=['duplicates']),final_df2[['tweet_id','random_coords']],on='tweet_id',how='left')\n",
    "    final_df.rename(columns={'final_coords':'org_coords','random_coords':'final_coords'},inplace=True)\n",
    "\n",
    "    \n",
    "    final_df = final_df.drop(['geo', 'coordinates', 'place', 'location_type', 'centers', 'geoparsing', 'geoparsing_count', 'user_location_lower', \n",
    "         'gp_coords', 'user_coordinates', 'gp_coords1', 'distance', \n",
    "         'final_coords_str'], axis=1)\n",
    "    \n",
    "    final_df['final_coords']=final_df['final_coords'].fillna(final_df['org_coords'])\n",
    "\n",
    "    final_df['lat'] = [c[0] for c in final_df['final_coords']]\n",
    "    final_df['lon'] = [c[1] for c in final_df['final_coords']]\n",
    "    \n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbNZjfDgAHDr"
   },
   "source": [
    "## Final algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eY_oWmB8AHDr"
   },
   "outputs": [],
   "source": [
    "def localization(df_rel,thres):\n",
    "    start1 = time.time()\n",
    "    df_rel = get_centers(df_rel)\n",
    "    df_rel = geoparsing(df_rel)\n",
    "    df_rel.to_json('geoparsed.json',orient='split')\n",
    "    \n",
    "    # look up table\n",
    "    start = time.time()\n",
    "    user_list = user_locs(df_rel,thres)\n",
    "    gp_list = geo_locs(df_rel,thres)    \n",
    "    df_lookup = lookup_table(user_list,gp_list)\n",
    "    end = time.time()\n",
    "    print('Lookup table DONE - ',(end-start)/60,' minutes')\n",
    "    \n",
    "    # add coordinates\n",
    "    start = time.time()\n",
    "    df_rel4 = add_coords(df_rel,df_lookup)    \n",
    "    df_rel5 = final_geocoords(df_rel4)\n",
    "    end = time.time()\n",
    "    print('Added coords DONE - ',(end-start)/60, ' minutes')\n",
    "\n",
    "    # get final coordinates\n",
    "    start = time.time()\n",
    "    final_df = final_data(df_rel5)\n",
    "    final_df2 = addnoise_coords(final_df)\n",
    "    end = time.time()\n",
    "    print('Final coords DONE - ',(end-start)/60, ' minutes')\n",
    "\n",
    "    # get only relevant columns\n",
    "    keep_cols = ['tweet_id','created_at','date','user_location','user_name','source','retweeted','type','retweet_count','hashtags','full_text','tweet','lat','lon','localization','org_coords','relevant_LR', 'relevant_RF',\n",
    "       'relevant_CNN', 'relevant_ULM']\n",
    "    final_df3 = final_df2[keep_cols]\n",
    "\n",
    "    end1 = time.time()\n",
    "\n",
    "    print('Complete time - ', (end1-start1)/60, ' minutes')\n",
    " \n",
    "    \n",
    "    return df_lookup,df_rel5, final_df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRgT86xRsdP_"
   },
   "source": [
    "### Use algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 492,
     "status": "ok",
     "timestamp": 1622038855844,
     "user": {
      "displayName": "Julie Petersen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq0qpPf2KC4BdN0egN2_TdTEglTbVVNL4VDKPWfw=s64",
      "userId": "00168236026826252484"
     },
     "user_tz": -120
    },
    "id": "YCsO4Btrskr6",
    "outputId": "5b91041f-f71d-40d3-a759-8199682d6854"
   },
   "outputs": [],
   "source": [
    "# get relevant tweets\n",
    "df_rel2 = df[df['relevant']==1]\n",
    "print('Relevant tweets:',len(df_rel2), '/', len(df))\n",
    "\n",
    "#subset\n",
    "size = len(df_rel2)\n",
    "df_rel = df_rel2.sample(size).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 258,
     "status": "ok",
     "timestamp": 1622038860678,
     "user": {
      "displayName": "Julie Petersen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq0qpPf2KC4BdN0egN2_TdTEglTbVVNL4VDKPWfw=s64",
      "userId": "00168236026826252484"
     },
     "user_tz": -120
    },
    "id": "6yTmK1brsnGt",
    "outputId": "079b2d44-b7cc-46ea-968a-014820f515f1"
   },
   "outputs": [],
   "source": [
    "geo_df = df_rel[~df_rel['geo'].isna()]\n",
    "#filter only relevant tweets\n",
    "geo_df = geo_df[geo_df['relevant']==1].reset_index(drop=True)\n",
    "geo_df['coords'] = [geo_df['geo'][i]['coordinates'] for i in range(len(geo_df))]\n",
    "print('Geotagged tweets:',len(geo_df), '/', len(df_rel))\n",
    "\n",
    "\n",
    "userloc_df = df_rel[~df_rel['user_location'].isna()]\n",
    "noloc_df = df_rel[(df_rel['user_location'].isna()) & (df_rel['geo'].isna())  & (df_rel['place'].isna()) ]\n",
    "noloc_df2 = df_rel[(df_rel['geo'].isna()) & (df_rel['place'].isna()) ]\n",
    "\n",
    "print(f'total : {len(df_rel)}')\n",
    "print(30*'_' + '\\n')\n",
    "\n",
    "print(f'geotagged : {len(geo_df)}')\n",
    "#print(f'places : {len(place_df2)}')\n",
    "print(f'user location : {len(userloc_df)}')\n",
    "print(f'no location : {len(noloc_df)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 983578,
     "status": "ok",
     "timestamp": 1622039848166,
     "user": {
      "displayName": "Julie Petersen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq0qpPf2KC4BdN0egN2_TdTEglTbVVNL4VDKPWfw=s64",
      "userId": "00168236026826252484"
     },
     "user_tz": -120
    },
    "id": "3i2QrioQAHDs",
    "outputId": "a1f9dde9-0f72-4d21-c348-7abfb6e61c2f"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "df_lookup,df_rel5, final_df = localization(df_rel,10)\n",
    "end = time.time()\n",
    "\n",
    "elapse = end-start\n",
    "print(elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t0navLQkNT3-"
   },
   "outputs": [],
   "source": [
    "#final_df.to_csv('final_tweets.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map tweets\n",
    "fig = px.scatter_geo(lat = final_df['lat'],lon=final_df['lon'],hover_name=final_df['tweet'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map look up table\n",
    "\n",
    "df_lookup['lat'] = [c[0] for c in df_lookup['coordinates']]\n",
    "df_lookup['lon'] = [c[1] for c in df_lookup['coordinates']]\n",
    "\n",
    "fig = px.scatter_geo(lat = df_lookup['lat'],lon=df_lookup['lon'],hover_name=df_lookup['location'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYt1XTAnAHDs"
   },
   "source": [
    "# Visualisation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T10:05:52.730892Z",
     "start_time": "2021-04-24T10:05:52.624382Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/eng_labelled_tweets.csv') # final_tweets, final_coords_tweets\n",
    "\n",
    "# filter only relevant tweets\n",
    "geo_df = df[~df['geo'].isna()].reset_index(drop=True)\n",
    "geo_df = geo_df[geo_df['relevant']==1].reset_index(drop=True)\n",
    "for i in range(len(geo_df)):\n",
    "    try: \n",
    "        geo_df['geo'][i] = eval(geo_df['geo'][i])\n",
    "    except:\n",
    "        geo_df['geo'][i] = geo_df['geo'][i]\n",
    "\n",
    "#geo_df['coords'] = [geo_df['geo'][i]['coordinates'] for i in range(len(geo_df))]\n",
    "geo_df['lat'] = [geo_df['geo'][i]['coordinates'][0] for i in range(len(geo_df))]\n",
    "geo_df['lon'] = [geo_df['geo'][i]['coordinates'][1] for i in range(len(geo_df))]\n",
    "\n",
    "#geo_df.to_csv('data/geo_tweets.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folium, Leaflet, OpenStreetMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base map\n",
    "m = folium.Map([20.416775, -3.70379], tiles=None, zoom_start=2)\n",
    "\n",
    "# tile layers\n",
    "folium.TileLayer('cartodbpositron', show=True, name=\"light\").add_to(m)\n",
    "folium.TileLayer('cartodbdark_matter', show=False, name=\"dark\").add_to(m)\n",
    "folium.TileLayer('openstreetmap', show=False, name=\"color\").add_to(m)\n",
    "\n",
    "# add location marker cluster\n",
    "mc = MarkerCluster(name='Tweets').add_to(m)\n",
    "\n",
    "# create marker at locations\n",
    "for lat, lon, user_location, full_text, created_at, retweet_count in zip(geo_df['lat'], geo_df['lon'], geo_df['user_location'], \n",
    "                                     geo_df['full_text'], geo_df['created_at'], geo_df['retweet_count']):\n",
    "    text = folium.Html('Tweet: {}<br> User location: {}<br> Created at: {}<br> Retweet count: {}<br>'.format(full_text, user_location, created_at, retweet_count), script=True)\n",
    "    popup = folium.Popup(text, max_width=300)\n",
    "    folium.CircleMarker(location = [lat, lon],\n",
    "                        radius = 2,\n",
    "                        weight = 5,\n",
    "                        color = '#081d58',\n",
    "                        fill_color = '#081d58',\n",
    "                        fill = True,\n",
    "                        popup = popup,\n",
    "                        tooltip = 'Click on Tweet'\n",
    "                        ).add_to(mc)\n",
    "mc.add_to(m)\n",
    "\n",
    "# add layer control\n",
    "folium.LayerControl('topright', collapsed=True).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kepler.gl\n",
    "\n",
    "https://medium.com/nightingale/how-to-create-eye-catching-maps-with-python-and-kepler-gl-e7e897eff8ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T12:10:52.502234Z",
     "start_time": "2021-04-24T12:10:52.439409Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "viz_cols = ['lon','lat','created_at','full_text','user_location']\n",
    "kepler_map = keplergl.KeplerGl(height=500)\n",
    "kepler_map.add_data(data=geo_df[viz_cols], name=\"Extreme weather events\")\n",
    "#kepler_map.save_to_html(file_name=\"kepler_map.html\")\n",
    "kepler_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T16:34:46.460910Z",
     "start_time": "2021-04-18T16:34:46.458398Z"
    }
   },
   "outputs": [],
   "source": [
    "#config = kepler_map.config\n",
    "#config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotly, Mapbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T10:06:54.820928Z",
     "start_time": "2021-04-24T10:06:54.744093Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# point map\n",
    "scatter_map = px.scatter_mapbox(geo_df, lat=\"lat\", lon=\"lon\", hover_name=\"full_text\", \n",
    "                        hover_data=[\"created_at\", \"user_location\",'retweeted'],\n",
    "                        color_discrete_sequence=[\"teal\"], \n",
    "                        zoom=1, height=500)\n",
    "scatter_map.update_layout(mapbox_style=\"carto-positron\",margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "scatter_map.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T11:01:45.233490Z",
     "start_time": "2021-04-24T11:01:45.163478Z"
    }
   },
   "outputs": [],
   "source": [
    "scatter_map = px.scatter_mapbox(\n",
    "    geo_df, lat=\"lat\", lon=\"lon\", \n",
    "    hover_data=['full_text',\"created_at\",\"user_location\"],\n",
    "    color = 'retweet_count',\n",
    "    color_continuous_scale='teal',\n",
    "    zoom=1, height=500)\n",
    "scatter_map.update_layout(mapbox_style=\"dark\",\n",
    "                          margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "scatter_map.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T10:06:59.078504Z",
     "start_time": "2021-04-24T10:06:58.996026Z"
    }
   },
   "outputs": [],
   "source": [
    "# size by retweet_count\n",
    "scatter_map = px.scatter_mapbox(\n",
    "    geo_df, lat=\"lat\", lon=\"lon\", \n",
    "    size = 'retweet_count',\n",
    "    size_max = 15,\n",
    "    color='retweeted',\n",
    "    hover_data=['full_text'],\n",
    "    #color_discrete_sequence=[\"teal\",\"\"],\n",
    "    zoom=1, height=500)\n",
    "scatter_map.update_layout(mapbox_style=\"dark\",\n",
    "                          margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "scatter_map.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T10:07:50.620070Z",
     "start_time": "2021-04-24T10:07:50.511192Z"
    }
   },
   "outputs": [],
   "source": [
    "hexabin_map = ff.create_hexbin_mapbox(data_frame=geo_df[['lat','lon']], lat=\"lat\", lon=\"lon\",\n",
    "                                      nx_hexagon=25, opacity=0.5, labels={\"color\": \"Relevant Tweets\"},\n",
    "                                      min_count=1, color_continuous_scale=\"Teal\",\n",
    "                                      show_original_data=True, height=500, zoom=0.95,\n",
    "                                      original_data_marker=dict(size=5, opacity=0.7, color=\"Teal\")\n",
    ")\n",
    "hexabin_map.update_layout(mapbox_style=\"carto-positron\",\n",
    "                          margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "hexabin_map.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T10:08:10.633292Z",
     "start_time": "2021-04-24T10:08:10.608593Z"
    }
   },
   "outputs": [],
   "source": [
    "# find number of tweets by date\n",
    "df['Date'] = pd.to_datetime(df['created_at']).dt.date\n",
    "count_dates = df.groupby('Date').size().values\n",
    "time_df = df.drop_duplicates(subset=\"Date\").assign(Count=count_dates)\n",
    "time_df = time_df[['Date','Count']].sort_values(by='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T10:08:14.481162Z",
     "start_time": "2021-04-24T10:08:14.392364Z"
    }
   },
   "outputs": [],
   "source": [
    "line_fig = px.line(time_df, x='Date', y='Count', title='Relevant Tweets over time')\n",
    "\n",
    "line_fig.update_xaxes(\n",
    "    rangeslider_visible=True,\n",
    "    rangeselector=dict(\n",
    "        buttons=list([\n",
    "            dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n",
    "            #dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n",
    "            #dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n",
    "            dict(step=\"all\")\n",
    "        ])\n",
    "    )\n",
    ")\n",
    "line_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotly, Treemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T19:31:22.066272Z",
     "start_time": "2021-05-02T19:31:21.405039Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/final_tweets.csv')\n",
    "df.dropna(subset=['tokens'],inplace=True)\n",
    "df['tokens'] = [literal_eval(s) for s in df['tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T20:02:04.466829Z",
     "start_time": "2021-05-02T20:02:04.343608Z"
    }
   },
   "outputs": [],
   "source": [
    "token_list = df['tokens'].tolist()\n",
    "token_list = [token for sublist in token_list for token in sublist]\n",
    "\n",
    "freq = FreqDist(token_list)\n",
    "freq_df = pd.DataFrame(list(freq.items()), columns = [\"Word\",\"Occurrences\"]) \n",
    "freq_df = freq_df[freq_df['Word']!=' ']\n",
    "freq_df = freq_df[freq_df['Word']!='']\n",
    "freq_df = freq_df.sort_values('Occurrences',ascending=False)\n",
    "freq_df[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T20:32:58.469643Z",
     "start_time": "2021-05-02T20:32:58.452221Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = go.Figure(go.Treemap(\n",
    "    labels=freq_df['Word'][:20].tolist(),\n",
    "    values=freq_df['Occurrences'][:20].tolist(),\n",
    "    parents=['']*20,\n",
    "    marker_colorscale=px.colors.sequential.Teal, # Burg, Darkmint, Mint, PuBu, Teal, YlGnBu, deep, ice, tempo\n",
    "    hovertemplate='<b>%{label} </b> <br> Occurrences: %{value}<extra></extra>',\n",
    "    pathbar={\"visible\": False},\n",
    "))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dash application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T09:06:06.126817Z",
     "start_time": "2021-06-01T09:06:04.895326Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('data/eng_merged.json',orient='split')\n",
    "df.rename(columns={'relevant_LR':'Logistic regression','relevant_RF':'Random forest','relevant_CNN':'CNN','relevant_ULM':'ULMFiT'},inplace=True)\n",
    "df = df[df['date'].dt.year>2016]\n",
    "df.drop(['tweet_id','retweeted','tweet'], axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T09:06:09.516979Z",
     "start_time": "2021-06-01T09:06:08.539549Z"
    }
   },
   "outputs": [],
   "source": [
    "#df.to_json('data/eng_tweets_1718.json',orient=\"split\")\n",
    "df = pd.read_json('data/eng_tweets_1718.json',orient=\"split\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate app\n",
    "app = JupyterDash(\n",
    "    __name__, external_stylesheets=[dbc.themes.BOOTSTRAP],\n",
    "    meta_tags=[{\"name\":\"viewport\",\"content\":\"width=device-width,initial-scale=1,maximum-scale=1.0,user-scalable=no\"}]\n",
    ")\n",
    "\n",
    "server = app.server\n",
    "app.config.suppress_callback_exceptions = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/s153748/extreme-weather-detection/blob/main/app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.run_server(mode='external', port=8060, use_reloader=False, debug=True)\n",
    "#app._terminate_server_for_port(\"localhost\", 8060)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUQQ3nAGAHDt"
   },
   "source": [
    "# Evaluation of Pipeline\n",
    "\n",
    "In this section we load the English unlabelled tweets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M99WUR_pAHDu"
   },
   "source": [
    "## Unlabelled tweets\n",
    "\n",
    "Load and clean unlabbeled tweets.\n",
    "This was done for one year at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SmaRAfm2AHDu"
   },
   "outputs": [],
   "source": [
    "# load unlabelled tweets\n",
    "years = ['2016','2017','2018']\n",
    "year = years[0]\n",
    "\n",
    "df1 = pd.read_json(f'Unlabbeled/eng_{year}.json', lines=True)\n",
    "\n",
    "# add userinfo, tokens, retweet variables etc.\n",
    "df2 = get_userinfo(df1)\n",
    "df3 = get_tokens(df2)\n",
    "df4 = add_vars(df3)\n",
    "\n",
    "df_new=df4\n",
    "\n",
    "# save to file\n",
    "df_new.to_json('new.json',orient='split')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnSE-xzFAHDu"
   },
   "source": [
    "## Classify tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_Uv2sqsAHDv"
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_relevance(new_tweet,model,clf_model):     \n",
    "    if len(new_tweet)==1:\n",
    "        rel = clf_model.predict(model.transform([new_tweet]))[0]\n",
    "    else:\n",
    "        rel = clf_model.predict(model.transform(new_tweet))\n",
    "      \n",
    "    return rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4169,
     "status": "ok",
     "timestamp": 1622456343542,
     "user": {
      "displayName": "Julie Petersen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq0qpPf2KC4BdN0egN2_TdTEglTbVVNL4VDKPWfw=s64",
      "userId": "00168236026826252484"
     },
     "user_tz": -120
    },
    "id": "Odqd_OK3AHDv",
    "outputId": "a2f36f2d-eaa7-444f-fcaa-e2b7d262f23f"
   },
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "\n",
    "clf_LR = pickle.load(open('Classifiers/LR.sav', 'rb'))\n",
    "LR_tfidf =  pickle.load(open('Classifiers/LR_tfidf.sav', 'rb'))\n",
    "\n",
    "labels = predict_relevance(df_new['tokens'],LR_tfidf,clf_LR)\n",
    "df_new['relevant_LR'] = labels\n",
    "print('Relevant tweets:', df_new['relevant_LR'].sum()/len(df_new)*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFsinkzogc4F"
   },
   "source": [
    "### Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4262,
     "status": "ok",
     "timestamp": 1622456347798,
     "user": {
      "displayName": "Julie Petersen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq0qpPf2KC4BdN0egN2_TdTEglTbVVNL4VDKPWfw=s64",
      "userId": "00168236026826252484"
     },
     "user_tz": -120
    },
    "id": "nA8_u16Bgg53",
    "outputId": "8242eecf-6715-4c39-b17d-8b6c0521f4b3"
   },
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "\n",
    "clf_RF = pickle.load(open('Classifiers/RF.sav', 'rb'))\n",
    "RF_tfidf =  pickle.load(open('Classifiers/RF_tfidf.sav', 'rb'))\n",
    "\n",
    "labels = predict_relevance(df_new['tokens'],RF_tfidf,clf_RF)\n",
    "df_new['relevant_RF'] = labels\n",
    "print('Relevant tweets:', df_new['relevant_RF'].sum()/len(df_new)*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GvfLeNY6xZS"
   },
   "source": [
    "### CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dYha17DQQMxT"
   },
   "outputs": [],
   "source": [
    "def predictions(df,model,tokenizer):\n",
    "\n",
    "    #tokenizer.fit_on_texts(df['full_text'].tolist())\n",
    "    sequences = tokenizer.texts_to_sequences(df['text_DL'])\n",
    "\n",
    "    MAX_SEQUENCE_LENGTH  = 30\n",
    "    new_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    predictions =model.predict_classes(new_data, batch_size=1024, verbose=1)\n",
    "\n",
    "    prediction_labels =  [i[0] for i in predictions.tolist()]\n",
    "            \n",
    "    return  prediction_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11127,
     "status": "ok",
     "timestamp": 1622456358911,
     "user": {
      "displayName": "Julie Petersen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq0qpPf2KC4BdN0egN2_TdTEglTbVVNL4VDKPWfw=s64",
      "userId": "00168236026826252484"
     },
     "user_tz": -120
    },
    "id": "jh7Ypcoe60Gn",
    "outputId": "8e779bde-8c3b-4d02-e634-7488c7cdc80e"
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "\n",
    "json_file = open('Classifiers/CNN.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "CNN = model_from_json(loaded_model_json)\n",
    "\n",
    "# load tokenizer\n",
    "json_file = open('Classifiers/CNN_tokenizer.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "CNN_tokenizer = tokenizer_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "\n",
    "CNN.load_weights(\"Classifiers/CNN.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "CNN.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "\n",
    "# get labels\n",
    "labels = predictions(df_new,CNN,CNN_tokenizer)\n",
    "df_new['relevant_CNN'] = labels\n",
    "print('Relevant tweets:', df_new['relevant_CNN'].sum()/len(df_new)*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqlFK_RQAHDx"
   },
   "source": [
    "### ULMFiT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 641655,
     "status": "ok",
     "timestamp": 1622457037740,
     "user": {
      "displayName": "Julie Petersen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq0qpPf2KC4BdN0egN2_TdTEglTbVVNL4VDKPWfw=s64",
      "userId": "00168236026826252484"
     },
     "user_tz": -120
    },
    "id": "9J2TBuwrG97X",
    "outputId": "9eb1d7d1-a048-454f-d2ed-c3e5f754346e"
   },
   "outputs": [],
   "source": [
    "# load ULMFiT\n",
    "\n",
    "cl = load_learner('','Classifiers/ULMFiT.pkl')\n",
    "cl.data.add_test(df_new['text_TL'])\n",
    "preds, _ = cl.get_preds(ds_type=DatasetType.Test)\n",
    "predictions = np.argmax(preds, axis=1)\n",
    "\n",
    "df_new['relevant_ULM'] = predictions.tolist()\n",
    "print('Relevant tweets:', df_new['relevant_ULM'].sum()/len(df_new)*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJoTVirXhyHa"
   },
   "outputs": [],
   "source": [
    "# save to file \n",
    "#df_new.to_json('labelled.json',orient='split')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPf9rfShph1t"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1622457039753,
     "user": {
      "displayName": "Julie Petersen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq0qpPf2KC4BdN0egN2_TdTEglTbVVNL4VDKPWfw=s64",
      "userId": "00168236026826252484"
     },
     "user_tz": -120
    },
    "id": "fs12ib0fpkBA",
    "outputId": "d7d5817b-3460-4a6e-b93b-6ac0631d62b7"
   },
   "outputs": [],
   "source": [
    "print('Tweets:',len(df_new))\n",
    "print('Relevant tweets LR:', df_new['relevant_LR'].sum()/len(df_new)*100, '%')\n",
    "print('Relevant tweets RF:', df_new['relevant_RF'].sum()/len(df_new)*100, '%')\n",
    "print('Relevant tweets CNN:', df_new['relevant_CNN'].sum()/len(df_new)*100, '%')\n",
    "print('Relevant tweets ULMFiT:', df_new['relevant_ULM'].sum()/len(df_new)*100, '%')\n",
    "\n",
    "print(60*'_')\n",
    "\n",
    "df_new['sum_relevance'] = df_new[['relevant_LR','relevant_RF','relevant_CNN','relevant_ULM']].sum(axis=1)\n",
    "\n",
    "print('At least one:',(df_new['sum_relevance']>0).sum()/len(df_new)*100, '%') \n",
    "print('Relevant tweets from all classifiers:', (df_new['sum_relevance']==4).sum()/len(df_new)*100, '%')\n",
    "print('Irrelevant tweets from all classifiers:', (df_new['sum_relevance']==0).sum()/len(df_new)*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRnJXU5gAHEL"
   },
   "source": [
    "## Get location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1622462285280,
     "user": {
      "displayName": "Julie Petersen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq0qpPf2KC4BdN0egN2_TdTEglTbVVNL4VDKPWfw=s64",
      "userId": "00168236026826252484"
     },
     "user_tz": -120
    },
    "id": "GMntZ8DmAHEL",
    "outputId": "5ac2f13f-65ac-47b8-93b1-fbb4b4f1065b"
   },
   "outputs": [],
   "source": [
    "# get relevant tweets\n",
    "\n",
    "df_rel = df_new[df_new['sum_relevance']!=0]\n",
    "df_rel.drop(columns={'sum_relevance'},inplace=True)\n",
    "print('Relevant tweets:',len(df_rel), '/', len(df_new))\n",
    "\n",
    "\n",
    "# get geo coordinates\n",
    "geo_df = df_rel[~df_rel['geo'].isna()]\n",
    "\n",
    "#filter only relevant tweets\n",
    "geo_df = geo_df[geo_df['relevant_CNN']==1].reset_index(drop=True)\n",
    "geo_df['coords'] = [geo_df['geo'][i]['coordinates'] for i in range(len(geo_df))]\n",
    "\n",
    "print('Geotagged tweets:',len(geo_df), '/', len(df_rel))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5lHdPJ3TAHEM",
    "outputId": "4b20cf86-e4bb-4aa2-c5b7-975127924ac4"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "threshold = 20\n",
    "Look_up, df_last, final_df = localization(df_rel,threshold)\n",
    "\n",
    "\n",
    "# save to fole\n",
    "#final_df.to_json('eng_2015.json',orient='split')\n",
    "#Look_up.to_csv('lookup_2015.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQCollRDi0Ul"
   },
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val,count = np.unique(final_df['localization'],return_counts=True)\n",
    "N = len(final_df)\n",
    "print('Geotagged coordinates: ', count[0]/N*100, '%'  )\n",
    "print('Geotagged place: ', count[1]/N*100, '%' )\n",
    "print('Geoparsed from text: ', count[2]/N*100, '%'  )\n",
    "print('Registered user location: ', count[3]/N*100, '%'  )\n",
    "print('No location:', (1-N/df_rel)*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hfk5XvlKAHEM"
   },
   "source": [
    "## Visualize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Main_J.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "266.973px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
